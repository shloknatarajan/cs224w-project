2025-12-08 00:10:09,309 - Using device: cuda
2025-12-08 00:10:09,309 - Logging results to: logs/baselines_20251208_001009.log
2025-12-08 00:10:09,309 - Loading dataset ogbl-ddi...
2025-12-08 00:10:09,396 - Dataset loaded: 4267 nodes
2025-12-08 00:10:09,537 - Train pos edges: 1067911, Valid pos: 133489, Test pos: 133489
2025-12-08 00:10:09,537 - Valid neg edges: 101882, Test neg: 95599
2025-12-08 00:10:09,557 - Added self-loops: Total edges now = 1072178
2025-12-08 00:10:09,564 - 
================================================================================
2025-12-08 00:04:20,841 - Current Improvements from baseline:
2025-12-08 00:04:20,841 - ================================================================================
2025-12-08 00:04:20,841 - Changes from baseline:
2025-12-08 00:04:20,841 - 1. ‚úÖ Hard negative sampling enabled (use_hard_negatives=True) (wasn't done previously)
2025-12-08 00:04:20,841 - 2. ‚úÖ MLP decoder with multi-strategy (use_multi_strategy=True)
2025-12-08 00:04:20,841 - 3. ‚úÖ Per-model learning rate tuning:
2025-12-08 00:04:20,841 -    - GCN: 0.015 (up from 0.01) - leverage strong performance
2025-12-08 00:04:20,841 -    - GraphSAGE: 0.003 (down from 0.01) - fix underfitting
2025-12-08 00:04:20,841 -    - GraphTransformer: 0.005 (unchanged)
2025-12-08 00:04:20,841 -    - GAT: 0.005 (unchanged)
2025-12-08 00:04:20,841 - ================================================================================
================================================================================
2025-12-08 00:10:09,564 - Training Simple GCN Baseline
2025-12-08 00:10:09,564 - ================================================================================
2025-12-08 00:10:09,604 - Starting training for GCN-Baseline (epochs=200, lr=0.01, patience=20, hard_neg=False)
2025-12-08 00:10:09,604 - Model description: Graph Convolutional Network with spectral convolutions | hidden_dim=128, num_layers=2, dropout=0.5, decoder_dropout=0.3, decoder=simple
2025-12-08 00:10:09,604 - Memory optimization: batch_size=50000, gradient_accumulation=1
2025-12-08 00:10:09,606 - Initialized EMA with decay=0.999 for stable checkpointing
2025-12-08 00:10:11,715 - GCN-Baseline Epoch 0001/200 [Random Neg] | Loss: 1.4673 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 1) | LR: 0.010000 üî•
2025-12-08 00:10:18,320 - GCN-Baseline Epoch 0005/200 [Random Neg] | Loss: 1.3327 | Val Hits@20: 0.0000 | Test Hits@20: 0.0001 | Best Val: 0.0000 (epoch 5) | LR: 0.010000 üî•
2025-12-08 00:10:26,307 - GCN-Baseline Epoch 0010/200 [Random Neg] | Loss: 1.2280 | Val Hits@20: 0.0004 | Test Hits@20: 0.0003 | Best Val: 0.0004 (epoch 10) | LR: 0.010000 üî•
2025-12-08 00:10:34,350 - GCN-Baseline Epoch 0015/200 [Random Neg] | Loss: 1.1462 | Val Hits@20: 0.0011 | Test Hits@20: 0.0009 | Best Val: 0.0011 (epoch 15) | LR: 0.010000 üî•
2025-12-08 00:10:42,351 - GCN-Baseline Epoch 0020/200 [Random Neg] | Loss: 1.0507 | Val Hits@20: 0.0017 | Test Hits@20: 0.0013 | Best Val: 0.0017 (epoch 20) | LR: 0.010000 üî•
2025-12-08 00:10:50,262 - GCN-Baseline Epoch 0025/200 [Random Neg] | Loss: 0.9848 | Val Hits@20: 0.0018 | Test Hits@20: 0.0015 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 üî•
2025-12-08 00:10:58,492 - GCN-Baseline Epoch 0030/200 [Random Neg] | Loss: 0.9529 | Val Hits@20: 0.0015 | Test Hits@20: 0.0010 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:11:06,400 - GCN-Baseline Epoch 0035/200 [Random Neg] | Loss: 0.9469 | Val Hits@20: 0.0011 | Test Hits@20: 0.0009 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:11:14,329 - GCN-Baseline Epoch 0040/200 [Random Neg] | Loss: 0.9270 | Val Hits@20: 0.0010 | Test Hits@20: 0.0009 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:11:22,428 - GCN-Baseline Epoch 0045/200 [Random Neg] | Loss: 0.9242 | Val Hits@20: 0.0009 | Test Hits@20: 0.0009 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:11:30,342 - GCN-Baseline Epoch 0050/200 [Random Neg] | Loss: 0.9141 | Val Hits@20: 0.0008 | Test Hits@20: 0.0007 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:11:38,519 - GCN-Baseline Epoch 0055/200 [Random Neg] | Loss: 0.9109 | Val Hits@20: 0.0007 | Test Hits@20: 0.0007 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:11:46,540 - GCN-Baseline Epoch 0060/200 [Random Neg] | Loss: 0.8998 | Val Hits@20: 0.0007 | Test Hits@20: 0.0007 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:11:54,433 - GCN-Baseline Epoch 0065/200 [Random Neg] | Loss: 0.8957 | Val Hits@20: 0.0007 | Test Hits@20: 0.0006 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:12:02,530 - GCN-Baseline Epoch 0070/200 [Random Neg] | Loss: 0.8789 | Val Hits@20: 0.0007 | Test Hits@20: 0.0006 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:12:10,420 - GCN-Baseline Epoch 0075/200 [Random Neg] | Loss: 0.8789 | Val Hits@20: 0.0007 | Test Hits@20: 0.0006 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:12:18,600 - GCN-Baseline Epoch 0080/200 [Random Neg] | Loss: 0.8698 | Val Hits@20: 0.0007 | Test Hits@20: 0.0006 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:12:26,747 - GCN-Baseline Epoch 0085/200 [Random Neg] | Loss: 0.8749 | Val Hits@20: 0.0007 | Test Hits@20: 0.0006 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:12:34,569 - GCN-Baseline Epoch 0090/200 [Random Neg] | Loss: 0.8638 | Val Hits@20: 0.0006 | Test Hits@20: 0.0006 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:12:42,408 - GCN-Baseline Epoch 0095/200 [Random Neg] | Loss: 0.8494 | Val Hits@20: 0.0007 | Test Hits@20: 0.0006 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:12:50,473 - GCN-Baseline Epoch 0100/200 [Random Neg] | Loss: 0.8513 | Val Hits@20: 0.0007 | Test Hits@20: 0.0006 | Best Val: 0.0018 (epoch 25) | LR: 0.010000 
2025-12-08 00:12:58,316 - GCN-Baseline Epoch 0105/200 [Random Neg] | Loss: 0.8494 | Val Hits@20: 0.0007 | Test Hits@20: 0.0007 | Best Val: 0.0018 (epoch 25) | LR: 0.005000 
2025-12-08 00:13:06,054 - GCN-Baseline Epoch 0110/200 [Random Neg] | Loss: 0.8355 | Val Hits@20: 0.0007 | Test Hits@20: 0.0007 | Best Val: 0.0018 (epoch 25) | LR: 0.005000 
2025-12-08 00:13:14,022 - GCN-Baseline Epoch 0115/200 [Random Neg] | Loss: 0.8343 | Val Hits@20: 0.0008 | Test Hits@20: 0.0007 | Best Val: 0.0018 (epoch 25) | LR: 0.005000 
2025-12-08 00:13:22,079 - GCN-Baseline Epoch 0120/200 [Random Neg] | Loss: 0.8269 | Val Hits@20: 0.0008 | Test Hits@20: 0.0007 | Best Val: 0.0018 (epoch 25) | LR: 0.005000 
2025-12-08 00:13:29,906 - GCN-Baseline Epoch 0125/200 [Random Neg] | Loss: 0.8264 | Val Hits@20: 0.0009 | Test Hits@20: 0.0006 | Best Val: 0.0018 (epoch 25) | LR: 0.005000 
2025-12-08 00:13:29,907 - GCN-Baseline: Early stopping at epoch 125 (no improvement for 20 eval steps)
2025-12-08 00:13:29,907 - GCN-Baseline FINAL: Best Val Hits@20 = 0.0018 | Test Hits@20 = 0.0015 (at epoch 25)
2025-12-08 00:13:29,907 - 
================================================================================
2025-12-08 00:13:29,907 - Training GraphSAGE Baseline
2025-12-08 00:13:29,907 - ================================================================================
2025-12-08 00:13:29,952 - Starting training for GraphSAGE-Baseline (epochs=200, lr=0.01, patience=20, hard_neg=False)
2025-12-08 00:13:29,952 - Model description: GraphSAGE with mean aggregation and ReLU activation | hidden_dim=128, num_layers=2, dropout=0.5, decoder_dropout=0.3, decoder=simple
2025-12-08 00:13:29,952 - Memory optimization: batch_size=50000, gradient_accumulation=1
2025-12-08 00:13:29,953 - Initialized EMA with decay=0.999 for stable checkpointing
2025-12-08 00:13:31,593 - GraphSAGE-Baseline Epoch 0001/200 [Random Neg] | Loss: 1.4652 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-08 00:13:38,035 - GraphSAGE-Baseline Epoch 0005/200 [Random Neg] | Loss: 1.4065 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-08 00:13:45,935 - GraphSAGE-Baseline Epoch 0010/200 [Random Neg] | Loss: 1.2427 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-08 00:13:53,786 - GraphSAGE-Baseline Epoch 0015/200 [Random Neg] | Loss: 1.1730 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-08 00:14:01,805 - GraphSAGE-Baseline Epoch 0020/200 [Random Neg] | Loss: 1.1239 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-08 00:14:09,929 - GraphSAGE-Baseline Epoch 0025/200 [Random Neg] | Loss: 1.0743 | Val Hits@20: 0.0000 | Test Hits@20: 0.0001 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-08 00:14:17,710 - GraphSAGE-Baseline Epoch 0030/200 [Random Neg] | Loss: 1.0345 | Val Hits@20: 0.0000 | Test Hits@20: 0.0001 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-08 00:14:25,587 - GraphSAGE-Baseline Epoch 0035/200 [Random Neg] | Loss: 1.0048 | Val Hits@20: 0.0000 | Test Hits@20: 0.0002 | Best Val: 0.0000 (epoch 35) | LR: 0.010000 üî•
2025-12-08 00:14:33,491 - GraphSAGE-Baseline Epoch 0040/200 [Random Neg] | Loss: 0.9922 | Val Hits@20: 0.0000 | Test Hits@20: 0.0002 | Best Val: 0.0000 (epoch 40) | LR: 0.010000 üî•
2025-12-08 00:14:41,451 - GraphSAGE-Baseline Epoch 0045/200 [Random Neg] | Loss: 0.9752 | Val Hits@20: 0.0000 | Test Hits@20: 0.0002 | Best Val: 0.0000 (epoch 45) | LR: 0.010000 üî•
2025-12-08 00:14:49,263 - GraphSAGE-Baseline Epoch 0050/200 [Random Neg] | Loss: 0.9680 | Val Hits@20: 0.0001 | Test Hits@20: 0.0004 | Best Val: 0.0001 (epoch 50) | LR: 0.010000 üî•
2025-12-08 00:14:57,254 - GraphSAGE-Baseline Epoch 0055/200 [Random Neg] | Loss: 0.9612 | Val Hits@20: 0.0001 | Test Hits@20: 0.0005 | Best Val: 0.0001 (epoch 55) | LR: 0.010000 üî•
2025-12-08 00:15:05,326 - GraphSAGE-Baseline Epoch 0060/200 [Random Neg] | Loss: 0.9508 | Val Hits@20: 0.0002 | Test Hits@20: 0.0006 | Best Val: 0.0002 (epoch 60) | LR: 0.010000 üî•
2025-12-08 00:15:13,154 - GraphSAGE-Baseline Epoch 0065/200 [Random Neg] | Loss: 0.9436 | Val Hits@20: 0.0003 | Test Hits@20: 0.0006 | Best Val: 0.0003 (epoch 65) | LR: 0.010000 üî•
2025-12-08 00:15:21,342 - GraphSAGE-Baseline Epoch 0070/200 [Random Neg] | Loss: 0.9367 | Val Hits@20: 0.0003 | Test Hits@20: 0.0008 | Best Val: 0.0003 (epoch 70) | LR: 0.010000 üî•
2025-12-08 00:15:29,356 - GraphSAGE-Baseline Epoch 0075/200 [Random Neg] | Loss: 0.9331 | Val Hits@20: 0.0004 | Test Hits@20: 0.0005 | Best Val: 0.0004 (epoch 75) | LR: 0.010000 üî•
2025-12-08 00:15:37,344 - GraphSAGE-Baseline Epoch 0080/200 [Random Neg] | Loss: 0.9283 | Val Hits@20: 0.0005 | Test Hits@20: 0.0006 | Best Val: 0.0005 (epoch 80) | LR: 0.010000 üî•
2025-12-08 00:15:45,300 - GraphSAGE-Baseline Epoch 0085/200 [Random Neg] | Loss: 0.9237 | Val Hits@20: 0.0006 | Test Hits@20: 0.0007 | Best Val: 0.0006 (epoch 85) | LR: 0.010000 üî•
2025-12-08 00:15:53,234 - GraphSAGE-Baseline Epoch 0090/200 [Random Neg] | Loss: 0.9215 | Val Hits@20: 0.0008 | Test Hits@20: 0.0008 | Best Val: 0.0008 (epoch 90) | LR: 0.010000 üî•
2025-12-08 00:16:01,212 - GraphSAGE-Baseline Epoch 0095/200 [Random Neg] | Loss: 0.9169 | Val Hits@20: 0.0009 | Test Hits@20: 0.0009 | Best Val: 0.0009 (epoch 95) | LR: 0.010000 üî•
2025-12-08 00:16:09,136 - GraphSAGE-Baseline Epoch 0100/200 [Random Neg] | Loss: 0.9155 | Val Hits@20: 0.0011 | Test Hits@20: 0.0009 | Best Val: 0.0011 (epoch 100) | LR: 0.010000 üî•
2025-12-08 00:16:17,130 - GraphSAGE-Baseline Epoch 0105/200 [Random Neg] | Loss: 0.9034 | Val Hits@20: 0.0014 | Test Hits@20: 0.0010 | Best Val: 0.0014 (epoch 105) | LR: 0.010000 üî•
2025-12-08 00:16:25,200 - GraphSAGE-Baseline Epoch 0110/200 [Random Neg] | Loss: 0.9043 | Val Hits@20: 0.0016 | Test Hits@20: 0.0011 | Best Val: 0.0016 (epoch 110) | LR: 0.010000 üî•
2025-12-08 00:16:33,111 - GraphSAGE-Baseline Epoch 0115/200 [Random Neg] | Loss: 0.9005 | Val Hits@20: 0.0016 | Test Hits@20: 0.0011 | Best Val: 0.0016 (epoch 115) | LR: 0.010000 üî•
2025-12-08 00:16:40,981 - GraphSAGE-Baseline Epoch 0120/200 [Random Neg] | Loss: 0.8973 | Val Hits@20: 0.0019 | Test Hits@20: 0.0012 | Best Val: 0.0019 (epoch 120) | LR: 0.010000 üî•
2025-12-08 00:16:48,874 - GraphSAGE-Baseline Epoch 0125/200 [Random Neg] | Loss: 0.8919 | Val Hits@20: 0.0020 | Test Hits@20: 0.0012 | Best Val: 0.0020 (epoch 125) | LR: 0.010000 üî•
2025-12-08 00:16:56,844 - GraphSAGE-Baseline Epoch 0130/200 [Random Neg] | Loss: 0.8939 | Val Hits@20: 0.0021 | Test Hits@20: 0.0012 | Best Val: 0.0021 (epoch 130) | LR: 0.010000 üî•
2025-12-08 00:17:04,854 - GraphSAGE-Baseline Epoch 0135/200 [Random Neg] | Loss: 0.8941 | Val Hits@20: 0.0021 | Test Hits@20: 0.0011 | Best Val: 0.0021 (epoch 135) | LR: 0.010000 üî•
2025-12-08 00:17:12,720 - GraphSAGE-Baseline Epoch 0140/200 [Random Neg] | Loss: 0.8875 | Val Hits@20: 0.0021 | Test Hits@20: 0.0011 | Best Val: 0.0021 (epoch 135) | LR: 0.010000 
2025-12-08 00:17:20,645 - GraphSAGE-Baseline Epoch 0145/200 [Random Neg] | Loss: 0.8843 | Val Hits@20: 0.0018 | Test Hits@20: 0.0011 | Best Val: 0.0021 (epoch 135) | LR: 0.010000 
2025-12-08 00:17:28,627 - GraphSAGE-Baseline Epoch 0150/200 [Random Neg] | Loss: 0.8825 | Val Hits@20: 0.0019 | Test Hits@20: 0.0011 | Best Val: 0.0021 (epoch 135) | LR: 0.010000 
2025-12-08 00:17:36,509 - GraphSAGE-Baseline Epoch 0155/200 [Random Neg] | Loss: 0.8759 | Val Hits@20: 0.0020 | Test Hits@20: 0.0012 | Best Val: 0.0021 (epoch 135) | LR: 0.010000 
2025-12-08 00:17:44,462 - GraphSAGE-Baseline Epoch 0160/200 [Random Neg] | Loss: 0.8733 | Val Hits@20: 0.0022 | Test Hits@20: 0.0012 | Best Val: 0.0022 (epoch 160) | LR: 0.010000 üî•
2025-12-08 00:17:52,256 - GraphSAGE-Baseline Epoch 0165/200 [Random Neg] | Loss: 0.8700 | Val Hits@20: 0.0025 | Test Hits@20: 0.0013 | Best Val: 0.0025 (epoch 165) | LR: 0.010000 üî•
2025-12-08 00:18:00,124 - GraphSAGE-Baseline Epoch 0170/200 [Random Neg] | Loss: 0.8710 | Val Hits@20: 0.0025 | Test Hits@20: 0.0016 | Best Val: 0.0025 (epoch 170) | LR: 0.010000 üî•
2025-12-08 00:18:08,211 - GraphSAGE-Baseline Epoch 0175/200 [Random Neg] | Loss: 0.8649 | Val Hits@20: 0.0026 | Test Hits@20: 0.0017 | Best Val: 0.0026 (epoch 175) | LR: 0.010000 üî•
2025-12-08 00:18:16,284 - GraphSAGE-Baseline Epoch 0180/200 [Random Neg] | Loss: 0.8660 | Val Hits@20: 0.0029 | Test Hits@20: 0.0017 | Best Val: 0.0029 (epoch 180) | LR: 0.010000 üî•
2025-12-08 00:18:24,045 - GraphSAGE-Baseline Epoch 0185/200 [Random Neg] | Loss: 0.8612 | Val Hits@20: 0.0029 | Test Hits@20: 0.0017 | Best Val: 0.0029 (epoch 185) | LR: 0.010000 üî•
2025-12-08 00:18:31,907 - GraphSAGE-Baseline Epoch 0190/200 [Random Neg] | Loss: 0.8584 | Val Hits@20: 0.0031 | Test Hits@20: 0.0016 | Best Val: 0.0031 (epoch 190) | LR: 0.010000 üî•
2025-12-08 00:18:39,876 - GraphSAGE-Baseline Epoch 0195/200 [Random Neg] | Loss: 0.8585 | Val Hits@20: 0.0031 | Test Hits@20: 0.0015 | Best Val: 0.0031 (epoch 190) | LR: 0.010000 
2025-12-08 00:18:47,773 - GraphSAGE-Baseline Epoch 0200/200 [Random Neg] | Loss: 0.8567 | Val Hits@20: 0.0033 | Test Hits@20: 0.0017 | Best Val: 0.0033 (epoch 200) | LR: 0.010000 üî•
2025-12-08 00:18:47,773 - GraphSAGE-Baseline FINAL: Best Val Hits@20 = 0.0033 | Test Hits@20 = 0.0017 (at epoch 200)
2025-12-08 00:18:47,773 - 
================================================================================
2025-12-08 00:18:47,773 - Training GraphTransformer Baseline
2025-12-08 00:18:47,774 - ================================================================================
2025-12-08 00:18:47,812 - Starting training for GraphTransformer-Baseline (epochs=200, lr=0.005, patience=20, hard_neg=False)
2025-12-08 00:18:47,812 - Model description: Graph Transformer with multi-head self-attention and position-aware edge encoding | hidden_dim=128, num_layers=2, heads=4, dropout=0.5, decoder_dropout=0.3, decoder=simple
2025-12-08 00:18:47,813 - Memory optimization: batch_size=50000, gradient_accumulation=1
2025-12-08 00:18:47,814 - Initialized EMA with decay=0.999 for stable checkpointing
2025-12-08 00:18:49,771 - GraphTransformer-Baseline Epoch 0001/200 [Random Neg] | Loss: 1.4677 | Val Hits@20: 0.0001 | Test Hits@20: 0.0001 | Best Val: 0.0001 (epoch 1) | LR: 0.005000 üî•
2025-12-08 00:18:56,590 - GraphTransformer-Baseline Epoch 0005/200 [Random Neg] | Loss: 1.4119 | Val Hits@20: 0.0001 | Test Hits@20: 0.0001 | Best Val: 0.0001 (epoch 5) | LR: 0.005000 üî•
2025-12-08 00:19:05,011 - GraphTransformer-Baseline Epoch 0010/200 [Random Neg] | Loss: 1.3812 | Val Hits@20: 0.0001 | Test Hits@20: 0.0001 | Best Val: 0.0001 (epoch 5) | LR: 0.005000 
2025-12-08 00:19:13,561 - GraphTransformer-Baseline Epoch 0015/200 [Random Neg] | Loss: 1.2762 | Val Hits@20: 0.0000 | Test Hits@20: 0.0001 | Best Val: 0.0001 (epoch 5) | LR: 0.005000 
2025-12-08 00:19:22,022 - GraphTransformer-Baseline Epoch 0020/200 [Random Neg] | Loss: 1.2141 | Val Hits@20: 0.0000 | Test Hits@20: 0.0001 | Best Val: 0.0001 (epoch 5) | LR: 0.005000 
2025-12-08 00:19:30,372 - GraphTransformer-Baseline Epoch 0025/200 [Random Neg] | Loss: 1.1626 | Val Hits@20: 0.0001 | Test Hits@20: 0.0001 | Best Val: 0.0001 (epoch 25) | LR: 0.005000 üî•
2025-12-08 00:19:38,826 - GraphTransformer-Baseline Epoch 0030/200 [Random Neg] | Loss: 1.1163 | Val Hits@20: 0.0001 | Test Hits@20: 0.0001 | Best Val: 0.0001 (epoch 30) | LR: 0.005000 üî•
2025-12-08 00:19:47,075 - GraphTransformer-Baseline Epoch 0035/200 [Random Neg] | Loss: 1.0899 | Val Hits@20: 0.0002 | Test Hits@20: 0.0001 | Best Val: 0.0002 (epoch 35) | LR: 0.005000 üî•
2025-12-08 00:19:55,863 - GraphTransformer-Baseline Epoch 0040/200 [Random Neg] | Loss: 1.0742 | Val Hits@20: 0.0002 | Test Hits@20: 0.0001 | Best Val: 0.0002 (epoch 40) | LR: 0.005000 üî•
2025-12-08 00:20:04,307 - GraphTransformer-Baseline Epoch 0045/200 [Random Neg] | Loss: 1.0594 | Val Hits@20: 0.0002 | Test Hits@20: 0.0002 | Best Val: 0.0002 (epoch 40) | LR: 0.005000 
2025-12-08 00:20:12,670 - GraphTransformer-Baseline Epoch 0050/200 [Random Neg] | Loss: 1.0535 | Val Hits@20: 0.0002 | Test Hits@20: 0.0003 | Best Val: 0.0002 (epoch 50) | LR: 0.005000 üî•
2025-12-08 00:20:20,912 - GraphTransformer-Baseline Epoch 0055/200 [Random Neg] | Loss: 1.0441 | Val Hits@20: 0.0002 | Test Hits@20: 0.0004 | Best Val: 0.0002 (epoch 50) | LR: 0.005000 
2025-12-08 00:20:28,467 - GraphTransformer-Baseline Epoch 0060/200 [Random Neg] | Loss: 1.0335 | Val Hits@20: 0.0002 | Test Hits@20: 0.0004 | Best Val: 0.0002 (epoch 60) | LR: 0.005000 üî•
2025-12-08 00:20:35,997 - GraphTransformer-Baseline Epoch 0065/200 [Random Neg] | Loss: 1.0228 | Val Hits@20: 0.0002 | Test Hits@20: 0.0006 | Best Val: 0.0002 (epoch 60) | LR: 0.005000 
2025-12-08 00:20:43,533 - GraphTransformer-Baseline Epoch 0070/200 [Random Neg] | Loss: 1.0106 | Val Hits@20: 0.0002 | Test Hits@20: 0.0007 | Best Val: 0.0002 (epoch 60) | LR: 0.005000 
2025-12-08 00:20:51,063 - GraphTransformer-Baseline Epoch 0075/200 [Random Neg] | Loss: 1.0000 | Val Hits@20: 0.0002 | Test Hits@20: 0.0008 | Best Val: 0.0002 (epoch 60) | LR: 0.005000 
2025-12-08 00:20:58,602 - GraphTransformer-Baseline Epoch 0080/200 [Random Neg] | Loss: 0.9831 | Val Hits@20: 0.0002 | Test Hits@20: 0.0009 | Best Val: 0.0002 (epoch 60) | LR: 0.005000 
2025-12-08 00:21:06,140 - GraphTransformer-Baseline Epoch 0085/200 [Random Neg] | Loss: 0.9655 | Val Hits@20: 0.0002 | Test Hits@20: 0.0009 | Best Val: 0.0002 (epoch 85) | LR: 0.005000 üî•
2025-12-08 00:21:13,708 - GraphTransformer-Baseline Epoch 0090/200 [Random Neg] | Loss: 0.9550 | Val Hits@20: 0.0003 | Test Hits@20: 0.0010 | Best Val: 0.0003 (epoch 90) | LR: 0.005000 üî•
2025-12-08 00:21:21,247 - GraphTransformer-Baseline Epoch 0095/200 [Random Neg] | Loss: 0.9564 | Val Hits@20: 0.0004 | Test Hits@20: 0.0011 | Best Val: 0.0004 (epoch 95) | LR: 0.005000 üî•
2025-12-08 00:21:28,792 - GraphTransformer-Baseline Epoch 0100/200 [Random Neg] | Loss: 0.9429 | Val Hits@20: 0.0003 | Test Hits@20: 0.0013 | Best Val: 0.0004 (epoch 95) | LR: 0.005000 
2025-12-08 00:21:36,325 - GraphTransformer-Baseline Epoch 0105/200 [Random Neg] | Loss: 0.9440 | Val Hits@20: 0.0003 | Test Hits@20: 0.0011 | Best Val: 0.0004 (epoch 95) | LR: 0.005000 
2025-12-08 00:21:43,885 - GraphTransformer-Baseline Epoch 0110/200 [Random Neg] | Loss: 0.9380 | Val Hits@20: 0.0003 | Test Hits@20: 0.0011 | Best Val: 0.0004 (epoch 95) | LR: 0.005000 
2025-12-08 00:21:51,433 - GraphTransformer-Baseline Epoch 0115/200 [Random Neg] | Loss: 0.9391 | Val Hits@20: 0.0003 | Test Hits@20: 0.0010 | Best Val: 0.0004 (epoch 95) | LR: 0.005000 
2025-12-08 00:21:59,046 - GraphTransformer-Baseline Epoch 0120/200 [Random Neg] | Loss: 0.9333 | Val Hits@20: 0.0003 | Test Hits@20: 0.0010 | Best Val: 0.0004 (epoch 95) | LR: 0.005000 
2025-12-08 00:22:06,592 - GraphTransformer-Baseline Epoch 0125/200 [Random Neg] | Loss: 0.9301 | Val Hits@20: 0.0003 | Test Hits@20: 0.0010 | Best Val: 0.0004 (epoch 95) | LR: 0.005000 
2025-12-08 00:22:14,114 - GraphTransformer-Baseline Epoch 0130/200 [Random Neg] | Loss: 0.9244 | Val Hits@20: 0.0003 | Test Hits@20: 0.0011 | Best Val: 0.0004 (epoch 95) | LR: 0.005000 
2025-12-08 00:22:21,691 - GraphTransformer-Baseline Epoch 0135/200 [Random Neg] | Loss: 0.9278 | Val Hits@20: 0.0004 | Test Hits@20: 0.0013 | Best Val: 0.0004 (epoch 135) | LR: 0.005000 üî•
2025-12-08 00:22:29,214 - GraphTransformer-Baseline Epoch 0140/200 [Random Neg] | Loss: 0.9207 | Val Hits@20: 0.0004 | Test Hits@20: 0.0013 | Best Val: 0.0004 (epoch 140) | LR: 0.005000 üî•
2025-12-08 00:22:36,753 - GraphTransformer-Baseline Epoch 0145/200 [Random Neg] | Loss: 0.9292 | Val Hits@20: 0.0004 | Test Hits@20: 0.0014 | Best Val: 0.0004 (epoch 145) | LR: 0.005000 üî•
2025-12-08 00:22:44,295 - GraphTransformer-Baseline Epoch 0150/200 [Random Neg] | Loss: 0.9237 | Val Hits@20: 0.0004 | Test Hits@20: 0.0014 | Best Val: 0.0004 (epoch 145) | LR: 0.005000 
2025-12-08 00:22:51,835 - GraphTransformer-Baseline Epoch 0155/200 [Random Neg] | Loss: 0.9247 | Val Hits@20: 0.0004 | Test Hits@20: 0.0015 | Best Val: 0.0004 (epoch 145) | LR: 0.005000 
2025-12-08 00:22:59,356 - GraphTransformer-Baseline Epoch 0160/200 [Random Neg] | Loss: 0.9218 | Val Hits@20: 0.0004 | Test Hits@20: 0.0016 | Best Val: 0.0004 (epoch 145) | LR: 0.005000 
2025-12-08 00:23:06,882 - GraphTransformer-Baseline Epoch 0165/200 [Random Neg] | Loss: 0.9202 | Val Hits@20: 0.0005 | Test Hits@20: 0.0017 | Best Val: 0.0005 (epoch 165) | LR: 0.005000 üî•
2025-12-08 00:23:14,423 - GraphTransformer-Baseline Epoch 0170/200 [Random Neg] | Loss: 0.9159 | Val Hits@20: 0.0004 | Test Hits@20: 0.0018 | Best Val: 0.0005 (epoch 165) | LR: 0.005000 
2025-12-08 00:23:21,933 - GraphTransformer-Baseline Epoch 0175/200 [Random Neg] | Loss: 0.9167 | Val Hits@20: 0.0004 | Test Hits@20: 0.0018 | Best Val: 0.0005 (epoch 165) | LR: 0.005000 
2025-12-08 00:23:29,445 - GraphTransformer-Baseline Epoch 0180/200 [Random Neg] | Loss: 0.9159 | Val Hits@20: 0.0003 | Test Hits@20: 0.0018 | Best Val: 0.0005 (epoch 165) | LR: 0.005000 
2025-12-08 00:23:36,966 - GraphTransformer-Baseline Epoch 0185/200 [Random Neg] | Loss: 0.9124 | Val Hits@20: 0.0003 | Test Hits@20: 0.0018 | Best Val: 0.0005 (epoch 165) | LR: 0.005000 
2025-12-08 00:23:44,492 - GraphTransformer-Baseline Epoch 0190/200 [Random Neg] | Loss: 0.9132 | Val Hits@20: 0.0003 | Test Hits@20: 0.0018 | Best Val: 0.0005 (epoch 165) | LR: 0.005000 
2025-12-08 00:23:52,029 - GraphTransformer-Baseline Epoch 0195/200 [Random Neg] | Loss: 0.9124 | Val Hits@20: 0.0003 | Test Hits@20: 0.0018 | Best Val: 0.0005 (epoch 165) | LR: 0.005000 
2025-12-08 00:23:59,525 - GraphTransformer-Baseline Epoch 0200/200 [Random Neg] | Loss: 0.9063 | Val Hits@20: 0.0004 | Test Hits@20: 0.0019 | Best Val: 0.0005 (epoch 165) | LR: 0.005000 
2025-12-08 00:23:59,525 - GraphTransformer-Baseline FINAL: Best Val Hits@20 = 0.0005 | Test Hits@20 = 0.0017 (at epoch 165)
2025-12-08 00:23:59,525 - 
================================================================================
2025-12-08 00:23:59,525 - Training GAT Baseline
2025-12-08 00:23:59,525 - ================================================================================
2025-12-08 00:23:59,686 - Starting training for GAT-Baseline (epochs=200, lr=0.005, patience=20, hard_neg=False)
2025-12-08 00:23:59,686 - Model description: Graph Attention Network with multi-head attention and ELU activation | hidden_dim=128, num_layers=2, heads=4, dropout=0.5, decoder_dropout=0.3, decoder=simple
2025-12-08 00:23:59,686 - Memory optimization: batch_size=50000, gradient_accumulation=1
2025-12-08 00:23:59,687 - Initialized EMA with decay=0.999 for stable checkpointing
2025-12-08 00:24:01,256 - GAT-Baseline Epoch 0001/200 [Random Neg] | Loss: 1.4669 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.005000 
2025-12-08 00:24:07,297 - GAT-Baseline Epoch 0005/200 [Random Neg] | Loss: 1.4123 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.005000 
2025-12-08 00:24:14,812 - GAT-Baseline Epoch 0010/200 [Random Neg] | Loss: 1.4042 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.005000 
2025-12-08 00:24:22,310 - GAT-Baseline Epoch 0015/200 [Random Neg] | Loss: 1.3970 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.005000 
2025-12-08 00:24:29,823 - GAT-Baseline Epoch 0020/200 [Random Neg] | Loss: 1.3138 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.005000 
2025-12-08 00:24:37,339 - GAT-Baseline Epoch 0025/200 [Random Neg] | Loss: 1.1985 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.005000 
2025-12-08 00:24:44,854 - GAT-Baseline Epoch 0030/200 [Random Neg] | Loss: 1.1389 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.005000 
2025-12-08 00:24:52,366 - GAT-Baseline Epoch 0035/200 [Random Neg] | Loss: 1.1330 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 35) | LR: 0.005000 üî•
2025-12-08 00:24:59,891 - GAT-Baseline Epoch 0040/200 [Random Neg] | Loss: 1.0997 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 35) | LR: 0.005000 
2025-12-08 00:25:07,437 - GAT-Baseline Epoch 0045/200 [Random Neg] | Loss: 1.0907 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 35) | LR: 0.005000 
2025-12-08 00:25:14,958 - GAT-Baseline Epoch 0050/200 [Random Neg] | Loss: 1.0494 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 50) | LR: 0.005000 üî•
2025-12-08 00:25:22,485 - GAT-Baseline Epoch 0055/200 [Random Neg] | Loss: 1.0263 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 55) | LR: 0.005000 üî•
2025-12-08 00:25:30,026 - GAT-Baseline Epoch 0060/200 [Random Neg] | Loss: 0.9972 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 60) | LR: 0.005000 üî•
2025-12-08 00:25:37,568 - GAT-Baseline Epoch 0065/200 [Random Neg] | Loss: 1.0219 | Val Hits@20: 0.0001 | Test Hits@20: 0.0000 | Best Val: 0.0001 (epoch 65) | LR: 0.005000 üî•
2025-12-08 00:25:45,093 - GAT-Baseline Epoch 0070/200 [Random Neg] | Loss: 0.9731 | Val Hits@20: 0.0001 | Test Hits@20: 0.0000 | Best Val: 0.0001 (epoch 70) | LR: 0.005000 üî•
2025-12-08 00:25:52,594 - GAT-Baseline Epoch 0075/200 [Random Neg] | Loss: 0.9684 | Val Hits@20: 0.0001 | Test Hits@20: 0.0000 | Best Val: 0.0001 (epoch 70) | LR: 0.005000 
2025-12-08 00:26:00,175 - GAT-Baseline Epoch 0080/200 [Random Neg] | Loss: 0.9582 | Val Hits@20: 0.0001 | Test Hits@20: 0.0000 | Best Val: 0.0001 (epoch 80) | LR: 0.005000 üî•
2025-12-08 00:26:07,719 - GAT-Baseline Epoch 0085/200 [Random Neg] | Loss: 0.9610 | Val Hits@20: 0.0001 | Test Hits@20: 0.0000 | Best Val: 0.0001 (epoch 85) | LR: 0.005000 üî•
2025-12-08 00:26:15,233 - GAT-Baseline Epoch 0090/200 [Random Neg] | Loss: 0.9744 | Val Hits@20: 0.0001 | Test Hits@20: 0.0000 | Best Val: 0.0001 (epoch 90) | LR: 0.005000 üî•
2025-12-08 00:26:22,752 - GAT-Baseline Epoch 0095/200 [Random Neg] | Loss: 0.9545 | Val Hits@20: 0.0002 | Test Hits@20: 0.0000 | Best Val: 0.0002 (epoch 95) | LR: 0.005000 üî•
2025-12-08 00:26:30,302 - GAT-Baseline Epoch 0100/200 [Random Neg] | Loss: 0.9489 | Val Hits@20: 0.0004 | Test Hits@20: 0.0001 | Best Val: 0.0004 (epoch 100) | LR: 0.005000 üî•
2025-12-08 00:26:37,822 - GAT-Baseline Epoch 0105/200 [Random Neg] | Loss: 0.9365 | Val Hits@20: 0.0006 | Test Hits@20: 0.0001 | Best Val: 0.0006 (epoch 105) | LR: 0.005000 üî•
2025-12-08 00:26:45,338 - GAT-Baseline Epoch 0110/200 [Random Neg] | Loss: 0.9506 | Val Hits@20: 0.0010 | Test Hits@20: 0.0002 | Best Val: 0.0010 (epoch 110) | LR: 0.005000 üî•
2025-12-08 00:26:52,867 - GAT-Baseline Epoch 0115/200 [Random Neg] | Loss: 0.9490 | Val Hits@20: 0.0014 | Test Hits@20: 0.0003 | Best Val: 0.0014 (epoch 115) | LR: 0.005000 üî•
2025-12-08 00:27:00,381 - GAT-Baseline Epoch 0120/200 [Random Neg] | Loss: 0.9287 | Val Hits@20: 0.0017 | Test Hits@20: 0.0005 | Best Val: 0.0017 (epoch 120) | LR: 0.005000 üî•
2025-12-08 00:27:07,922 - GAT-Baseline Epoch 0125/200 [Random Neg] | Loss: 0.9194 | Val Hits@20: 0.0018 | Test Hits@20: 0.0006 | Best Val: 0.0018 (epoch 125) | LR: 0.005000 üî•
2025-12-08 00:27:15,466 - GAT-Baseline Epoch 0130/200 [Random Neg] | Loss: 0.9199 | Val Hits@20: 0.0023 | Test Hits@20: 0.0007 | Best Val: 0.0023 (epoch 130) | LR: 0.005000 üî•
2025-12-08 00:27:22,987 - GAT-Baseline Epoch 0135/200 [Random Neg] | Loss: 0.9045 | Val Hits@20: 0.0026 | Test Hits@20: 0.0008 | Best Val: 0.0026 (epoch 135) | LR: 0.005000 üî•
2025-12-08 00:27:30,513 - GAT-Baseline Epoch 0140/200 [Random Neg] | Loss: 0.9162 | Val Hits@20: 0.0030 | Test Hits@20: 0.0009 | Best Val: 0.0030 (epoch 140) | LR: 0.005000 üî•
2025-12-08 00:27:38,034 - GAT-Baseline Epoch 0145/200 [Random Neg] | Loss: 0.9256 | Val Hits@20: 0.0030 | Test Hits@20: 0.0010 | Best Val: 0.0030 (epoch 140) | LR: 0.005000 
2025-12-08 00:27:45,557 - GAT-Baseline Epoch 0150/200 [Random Neg] | Loss: 0.9280 | Val Hits@20: 0.0029 | Test Hits@20: 0.0010 | Best Val: 0.0030 (epoch 140) | LR: 0.005000 
2025-12-08 00:27:53,079 - GAT-Baseline Epoch 0155/200 [Random Neg] | Loss: 0.9225 | Val Hits@20: 0.0030 | Test Hits@20: 0.0010 | Best Val: 0.0030 (epoch 155) | LR: 0.005000 üî•
2025-12-08 00:28:00,607 - GAT-Baseline Epoch 0160/200 [Random Neg] | Loss: 0.9297 | Val Hits@20: 0.0031 | Test Hits@20: 0.0010 | Best Val: 0.0031 (epoch 160) | LR: 0.005000 üî•
2025-12-08 00:28:08,135 - GAT-Baseline Epoch 0165/200 [Random Neg] | Loss: 0.9081 | Val Hits@20: 0.0031 | Test Hits@20: 0.0011 | Best Val: 0.0031 (epoch 160) | LR: 0.005000 
2025-12-08 00:28:15,678 - GAT-Baseline Epoch 0170/200 [Random Neg] | Loss: 0.9058 | Val Hits@20: 0.0030 | Test Hits@20: 0.0011 | Best Val: 0.0031 (epoch 160) | LR: 0.005000 
2025-12-08 00:28:23,214 - GAT-Baseline Epoch 0175/200 [Random Neg] | Loss: 0.8990 | Val Hits@20: 0.0030 | Test Hits@20: 0.0011 | Best Val: 0.0031 (epoch 160) | LR: 0.005000 
2025-12-08 00:28:30,729 - GAT-Baseline Epoch 0180/200 [Random Neg] | Loss: 0.8863 | Val Hits@20: 0.0030 | Test Hits@20: 0.0011 | Best Val: 0.0031 (epoch 160) | LR: 0.005000 
2025-12-08 00:28:38,229 - GAT-Baseline Epoch 0185/200 [Random Neg] | Loss: 0.8965 | Val Hits@20: 0.0030 | Test Hits@20: 0.0011 | Best Val: 0.0031 (epoch 160) | LR: 0.005000 
2025-12-08 00:28:45,757 - GAT-Baseline Epoch 0190/200 [Random Neg] | Loss: 0.8889 | Val Hits@20: 0.0031 | Test Hits@20: 0.0011 | Best Val: 0.0031 (epoch 190) | LR: 0.005000 üî•
2025-12-08 00:28:53,279 - GAT-Baseline Epoch 0195/200 [Random Neg] | Loss: 0.8856 | Val Hits@20: 0.0031 | Test Hits@20: 0.0011 | Best Val: 0.0031 (epoch 190) | LR: 0.005000 
2025-12-08 00:29:00,785 - GAT-Baseline Epoch 0200/200 [Random Neg] | Loss: 0.8823 | Val Hits@20: 0.0031 | Test Hits@20: 0.0011 | Best Val: 0.0031 (epoch 190) | LR: 0.005000 
2025-12-08 00:29:00,785 - GAT-Baseline FINAL: Best Val Hits@20 = 0.0031 | Test Hits@20 = 0.0011 (at epoch 190)
2025-12-08 00:29:00,786 - 
================================================================================
2025-12-08 00:29:00,786 - FINAL RESULTS - BASELINES
2025-12-08 00:29:00,786 - ================================================================================
2025-12-08 00:29:00,786 - GCN:
2025-12-08 00:29:00,786 -   Validation Hits@20: 0.0018
2025-12-08 00:29:00,786 -   Test Hits@20: 0.0015
2025-12-08 00:29:00,786 -   Val-Test Gap: 0.0003 (17.9% relative)
2025-12-08 00:29:00,786 - GraphSAGE:
2025-12-08 00:29:00,786 -   Validation Hits@20: 0.0033
2025-12-08 00:29:00,786 -   Test Hits@20: 0.0017
2025-12-08 00:29:00,786 -   Val-Test Gap: 0.0016 (48.8% relative)
2025-12-08 00:29:00,786 - GraphTransformer:
2025-12-08 00:29:00,786 -   Validation Hits@20: 0.0005
2025-12-08 00:29:00,786 -   Test Hits@20: 0.0017
2025-12-08 00:29:00,786 -   Val-Test Gap: -0.0012 (-267.7% relative)
2025-12-08 00:29:00,786 - GAT:
2025-12-08 00:29:00,787 -   Validation Hits@20: 0.0031
2025-12-08 00:29:00,787 -   Test Hits@20: 0.0011
2025-12-08 00:29:00,787 -   Val-Test Gap: 0.0020 (64.2% relative)
2025-12-08 00:29:00,787 - ================================================================================
2025-12-08 00:29:00,787 - Results logged to: logs/baselines_20251208_001009.log

================================================================================
POST-RUN ANALYSIS (Generated: 2025-12-08 00:51 UTC)
================================================================================

ARCHITECTURE EVOLUTION SUMMARY
-------------------------------------------------------------------------------
This run (Phase 1) attempted improvements over initial baseline (Dec 7, 23:28).

Run 1 - Initial Baseline (logs/initial_baselines_20251207_232853.log):
  - Decoder: Simple dot product (h_u ¬∑ h_v)
  - Negative Sampling: Random only (hard_neg=False)
  - Learning Rates: GCN/SAGE=0.01, Transformer/GAT=0.005
  - Results:
    * GCN:         0.57% val, 0.24% test (58.1% gap)
    * Transformer: 0.29% val, 0.24% test (16.1% gap)
    * GAT:         0.25% val, 0.13% test (46.7% gap)
    * SAGE:        0.15% val, 0.06% test (60.2% gap)

Run 2 - This Run (Phase 1 Improvements):
  - Decoder: Multi-strategy MLP (Hadamard + Concat + Bilinear)
  - Negative Sampling: Hard negatives (30% hard after epoch 50)
  - Learning Rates: GCN=0.015 (+50%), SAGE=0.003 (-70%), others=0.005
  - Results:
    * SAGE:        0.33% val, 0.17% test (48.8% gap) [+120% val ‚úì]
    * GAT:         0.31% val, 0.11% test (64.2% gap) [+24% val ‚úì]
    * GCN:         0.18% val, 0.15% test (17.9% gap) [-68% val ‚úó]
    * Transformer: 0.05% val, 0.17% test (-267.7% gap) [-83% val ‚úó]

IMPACT ASSESSMENT
-------------------------------------------------------------------------------
Change                  | Expected      | Actual Result
------------------------|---------------|----------------------------------
MLP Decoder             | +30-150%      | Mixed: 2 worse, 2 slightly better
Hard Negatives          | +50-200%      | Mixed: hurt GCN/Trans, helped SAGE/GAT
GCN LR ‚Üë (0.01‚Üí0.015)   | Faster conv   | -68% performance drop
SAGE LR ‚Üì (0.01‚Üí0.003)  | Fix underfit  | +120% improvement (but still 0.33%)

CRITICAL FINDINGS
-------------------------------------------------------------------------------
1. CATASTROPHIC PERFORMANCE (üö® HIGHEST PRIORITY)
   - All models: <0.6% Hits@20
   - Expected (OGB leaderboard):
     * State-of-the-art: ~90% Hits@20
     * Simple GCN baseline: ~30% Hits@20
     * Our best: 0.33% (100-300x worse!)
   - Random baseline: 1/101,883 = 0.001%
   - Our models: 0.05-0.33% (only 5-30x better than random!)

   ‚Üí CONCLUSION: This indicates fundamental bugs, not hyperparameter issues

2. EXTREME VARIANCE (üé≤ CRITICAL)
   - GCN: 0.57% ‚Üí 0.18% (-68%)
   - Transformer: 0.29% ‚Üí 0.05% (-83%)
   - SAGE: 0.15% ‚Üí 0.33% (+120%)

   ‚Üí Results are essentially random noise
   ‚Üí Cannot trust single runs
   ‚Üí Multi-seed evaluation mandatory

3. "IMPROVEMENTS" MADE THINGS WORSE (‚ö†Ô∏è)
   - 2 out of 4 models performed worse with "better" config
   - Net negative result overall (avg performance dropped)
   - Only SAGE improved, but absolute performance still terrible

   ‚Üí We don't understand what's broken
   ‚Üí Stop optimizing, start debugging

4. OVERFITTING WITHOUT LEARNING
   - Large val-test gaps persist (17-64%)
   - But absolute performance <0.6% suggests models aren't learning anything
   - Hypothesis: Memorizing random noise, not graph structure

5. TRAINING DYNAMICS ISSUES
   - GCN: Peaked epoch 25, declined to 125 (overfitting)
   - SAGE: Still improving at epoch 200 (needs more epochs?)
   - Transformer/GAT: Near-zero for first 50-100 epochs (init/LR issue?)

COMPARISON WITH PUBLISHED RESULTS
-------------------------------------------------------------------------------
OGB Leaderboard (ogbl-ddi):
  - Top models:      ~90% Hits@20
  - GCN baseline:    ~30% Hits@20
  - Simple heuristics: ~10-15% Hits@20

Our Results:
  - Best (initial):   0.57% Hits@20
  - Best (Phase 1):   0.33% Hits@20

Gap: 50-500x worse than expected ‚Üí THIS IS A BUG, NOT A TUNING ISSUE

HYPERPARAMETER SWEEP RECOMMENDATION
-------------------------------------------------------------------------------
ANSWER: NO - Do NOT sweep hyperparameters yet

Rationale:
  1. Implementation is broken (100-500x worse than expected)
  2. High variance makes results unreliable (¬±120% swings)
  3. "Improvements" made things worse (proves we don't understand root cause)
  4. Wasting compute on broken code
  5. Will find "optimal" configs for broken implementation

When to sweep (only after ALL of these):
  ‚úì Code correctness verified (embeddings, decoder, evaluation)
  ‚úì At least one model achieving >10% Hits@20 (20x current best)
  ‚úì Multi-seed variance is low (CV < 20%)
  ‚úì Clear understanding of what works and why

Then sweep:
  - Learning rate: [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03]
  - Hidden dim: [64, 128, 256, 512]
  - Layers: [2, 3, 4]
  - Dropout: [0.0, 0.1, 0.3, 0.5]
  - Use Bayesian optimization (Optuna, Ray Tune), not grid search

REQUIRED NEXT STEPS (PRIORITY ORDER)
-------------------------------------------------------------------------------
üî¥ CRITICAL (Do First):
  1. Debug entire pipeline:
     - Check embedding collapse (cosine similarity analysis)
     - Verify decoder distinguishes pos/neg edges (score distributions)
     - Validate evaluation metrics (compare with OGB evaluator)
     - Compare with OGB reference implementation

  2. Verify data integrity:
     - Confirm train/val/test splits correct
     - Check for data leakage
     - Verify negative sampling doesn't include positive edges

  3. Simplify to minimal working example:
     - 2-layer GCN only
     - Simple dot product decoder
     - No hard negatives, no fancy techniques
     - Fix random seeds
     - TARGET: >10% Hits@20 before proceeding

DIAGNOSTIC HYPOTHESES TO TEST
-------------------------------------------------------------------------------
Hypothesis 1: Embedding Collapse
  Test: Check if all embeddings are nearly identical
  Code: cosine_similarity(emb.unsqueeze(0), emb.unsqueeze(1))
  Expected: mean < 0.8, std > 0.1
  Fix: Reduce dropout, check initialization, add regularization

Hypothesis 2: Decoder Bug
  Test: Do positive edges score higher than negative?
  Code: pos_scores.mean() vs neg_scores.mean()
  Expected: pos_scores > neg_scores
  Fix: Reimplement decoder, verify gradient flow

Hypothesis 3: Evaluation Metric Bug
  Test: Manual Hits@20 vs logged value
  Fix: Use OGB official evaluator

Hypothesis 4: Data Leakage
  Test: Check negative edges don't overlap with val/test
  Fix: Fix negative sampling logic

Hypothesis 5: Wrong Loss/Objective
  Test: Loss decreases but validation doesn't improve
  Fix: Review loss implementation

DIAGNOSTIC CODE TO ADD
-------------------------------------------------------------------------------
def log_diagnostics(model, data, pos_edges, neg_edges, epoch):
    with torch.no_grad():
        # 1. Embedding stats
        emb = model.encode(data)
        emb_sim = torch.cosine_similarity(emb.unsqueeze(0), emb.unsqueeze(1), dim=2)

        # 2. Score distributions
        pos_scores = model.decode(pos_edges[:, :1000]).mean()
        neg_scores = model.decode(neg_edges[:, :1000]).mean()

        # 3. Gradient norm
        grad_norm = sum(p.grad.norm()**2 for p in model.parameters() if p.grad).sqrt()

        print(f"[Epoch {epoch}] emb_sim={emb_sim.mean():.3f}, "
              f"scores: pos={pos_scores:.3f} neg={neg_scores:.3f}, "
              f"grad_norm={grad_norm:.2f}")

        # Warnings
        if emb_sim.mean() > 0.8: print("  ‚ö†Ô∏è  EMBEDDING COLLAPSE")
        if pos_scores <= neg_scores: print("  üö® DECODER BUG")

