2025-12-09 01:43:37,768 - Device: cuda
2025-12-09 01:43:37,768 - Log file: logs/sage_variants_20251209_014337/variants.log
2025-12-09 01:43:37,768 - 
Loading ogbl-ddi dataset...
2025-12-09 01:43:37,768 - Loading dataset ogbl-ddi...
2025-12-09 01:43:37,853 - Dataset loaded: 4267 nodes
2025-12-09 01:43:37,997 - Train pos edges: 1067911, Valid pos: 133489, Test pos: 133489
2025-12-09 01:43:37,997 - Valid neg edges: 101882, Test neg: 95599
2025-12-09 01:43:38,017 - Added self-loops: Total edges now = 1072178
2025-12-09 01:43:38,024 - Nodes: 4267
2025-12-09 01:43:38,024 - Train: 1067911 edges
2025-12-09 01:43:38,025 - Valid: 133489 pos, 101882 neg
2025-12-09 01:43:38,025 - Test: 133489 pos, 95599 neg
2025-12-09 01:43:38,025 - 
================================================================================
2025-12-09 01:43:38,025 - GRAPHSAGE ARCHITECTURE VARIANTS - CONFIGURATION
2025-12-09 01:43:38,025 - ================================================================================
2025-12-09 01:43:38,025 - Fixed Settings (same as baseline):
2025-12-09 01:43:38,025 -   - Learning rate: 0.01
2025-12-09 01:43:38,025 -   - Hidden dim: 128
2025-12-09 01:43:38,025 -   - Epochs: 100
2025-12-09 01:43:38,025 -   - Batch size: 50000
2025-12-09 01:43:38,025 -   - Decoder: Simple dot product
2025-12-09 01:43:38,025 -   - Negative sampling: Random 1:1
2025-12-09 01:43:38,025 -   - NO: early stopping, hard negatives, weight decay
2025-12-09 01:43:38,025 - 
Variants to test:
2025-12-09 01:43:38,025 -   1. Baseline: 2 layers, no BatchNorm, dropout=0.0
2025-12-09 01:43:38,025 -   2. BatchNorm: 2 layers, BatchNorm, dropout=0.0
2025-12-09 01:43:38,025 -   3. Depth: 3 layers, no BatchNorm, dropout=0.0
2025-12-09 01:43:38,025 -   4. Both: 3 layers, BatchNorm, dropout=0.0
2025-12-09 01:43:38,025 - ================================================================================
2025-12-09 01:43:38,025 - 
================================================================================
2025-12-09 01:43:38,026 - Training Variant 1: Baseline
2025-12-09 01:43:38,026 - ================================================================================
2025-12-09 01:43:38,064 - Model: SAGE-V1-Baseline (2L, no-BN, drop=0.0) | hidden_dim=128, decoder=simple
2025-12-09 01:43:38,064 - Starting training for Variant 1: Baseline (epochs=100, lr=0.01, patience=None, hard_neg=False)
2025-12-09 01:43:38,064 - Model description: SAGE-V1-Baseline (2L, no-BN, drop=0.0) | hidden_dim=128, decoder=simple
2025-12-09 01:43:38,064 - Memory optimization: batch_size=50000, gradient_accumulation=1
2025-12-09 01:43:38,066 - Initialized EMA with decay=0.999 for stable checkpointing
2025-12-09 01:43:39,882 - Variant 1: Baseline Epoch 0001/100 [Random Neg] | Loss: 1.4653 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-09 01:43:40,289 - [DIAGNOSTICS Epoch 1] Emb: sim_mean=0.999 sim_std=0.001 norm=0.727 | Scores: pos=0.492 neg=0.492 gap=0.000 | Grad: norm=0.02
2025-12-09 01:43:40,289 -   ‚ö†Ô∏è  EMBEDDING COLLAPSE (similarity > 0.9)
2025-12-09 01:43:40,289 -   ‚ö†Ô∏è  LOW EMBEDDING DIVERSITY (std < 0.05)
2025-12-09 01:43:46,093 - Variant 1: Baseline Epoch 0005/100 [Random Neg] | Loss: 1.4070 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-09 01:43:46,454 - [DIAGNOSTICS Epoch 5] Emb: sim_mean=0.998 sim_std=0.004 norm=0.414 | Scores: pos=0.503 neg=0.502 gap=0.000 | Grad: norm=0.01
2025-12-09 01:43:46,454 -   ‚ö†Ô∏è  EMBEDDING COLLAPSE (similarity > 0.9)
2025-12-09 01:43:46,454 -   ‚ö†Ô∏è  LOW EMBEDDING DIVERSITY (std < 0.05)
2025-12-09 01:43:53,684 - Variant 1: Baseline Epoch 0010/100 [Random Neg] | Loss: 1.3527 | Val Hits@20: 0.0000 | Test Hits@20: 0.0000 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-09 01:43:54,038 - [DIAGNOSTICS Epoch 10] Emb: sim_mean=0.987 sim_std=0.058 norm=2.794 | Scores: pos=0.453 neg=0.363 gap=0.089 | Grad: norm=0.16
2025-12-09 01:43:54,038 -   ‚ö†Ô∏è  EMBEDDING COLLAPSE (similarity > 0.9)
2025-12-09 01:44:01,222 - Variant 1: Baseline Epoch 0015/100 [Random Neg] | Loss: 1.2819 | Val Hits@20: 0.0000 | Test Hits@20: 0.0001 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-09 01:44:08,418 - Variant 1: Baseline Epoch 0020/100 [Random Neg] | Loss: 1.1632 | Val Hits@20: 0.0000 | Test Hits@20: 0.0001 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-09 01:44:08,771 - [DIAGNOSTICS Epoch 20] Emb: sim_mean=0.914 sim_std=0.089 norm=4.343 | Scores: pos=0.561 neg=0.291 gap=0.269 | Grad: norm=0.30
2025-12-09 01:44:08,772 -   ‚ö†Ô∏è  EMBEDDING COLLAPSE (similarity > 0.9)
2025-12-09 01:44:15,972 - Variant 1: Baseline Epoch 0025/100 [Random Neg] | Loss: 1.0497 | Val Hits@20: 0.0000 | Test Hits@20: 0.0001 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-09 01:44:23,202 - Variant 1: Baseline Epoch 0030/100 [Random Neg] | Loss: 0.9736 | Val Hits@20: 0.0000 | Test Hits@20: 0.0001 | Best Val: 0.0000 (epoch 0) | LR: 0.010000 
2025-12-09 01:44:30,395 - Variant 1: Baseline Epoch 0035/100 [Random Neg] | Loss: 0.9204 | Val Hits@20: 0.0001 | Test Hits@20: 0.0002 | Best Val: 0.0001 (epoch 35) | LR: 0.010000 üî•
