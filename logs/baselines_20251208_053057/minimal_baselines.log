2025-12-08 05:30:57,652 - Device: cuda
2025-12-08 05:30:57,652 - Log file: logs/minimal_baselines_20251208_053057/minimal_baselines.log
2025-12-08 05:30:57,652 - 
Loading ogbl-ddi dataset...
2025-12-08 05:30:57,740 - Nodes: 4267
2025-12-08 05:30:57,902 - Train: 1067911 edges
2025-12-08 05:30:57,902 - Valid: 133489 pos, 101882 neg
2025-12-08 05:30:57,902 - Test: 133489 pos, 95599 neg
2025-12-08 05:30:57,925 - Graph: 1072178 edges (with self-loops)
2025-12-08 05:30:57,925 - 
================================================================================
2025-12-08 05:30:57,925 - BASELINES - CONFIGURATION
2025-12-08 05:30:57,925 - ================================================================================
2025-12-08 05:30:57,925 - Common Settings:
2025-12-08 05:30:57,925 -   - 2 layers per model
2025-12-08 05:30:57,925 -   - 128 hidden dim
2025-12-08 05:30:57,925 -   - Learnable embeddings only (NO structural features)
2025-12-08 05:30:57,925 -   - Dot product decoder (z_u Â· z_v)
2025-12-08 05:30:57,925 -   - Random 1:1 negative sampling
2025-12-08 05:30:57,925 -   - Adam optimizer (lr=0.01)
2025-12-08 05:30:57,926 -   - 100 epochs (no early stopping)
2025-12-08 05:30:57,926 -   - NO: hard negatives, diversity loss, multi-strategy, dropout
2025-12-08 05:30:57,926 - ================================================================================
2025-12-08 05:30:57,968 - 
================================================================================
2025-12-08 05:30:57,969 - Training GCN
2025-12-08 05:30:57,969 - ================================================================================
2025-12-08 05:30:58,344 - GCN Epoch 001/100 | Loss: 1.3856 | Val: 0.0153 | Test: 0.0144 | Best Val: 0.0153 (epoch 1) ðŸ”¥
2025-12-08 05:30:58,866 - GCN Epoch 010/100 | Loss: 1.2688 | Val: 0.0241 | Test: 0.0502 | Best Val: 0.0241 (epoch 10) ðŸ”¥
2025-12-08 05:30:59,443 - GCN Epoch 020/100 | Loss: 1.2332 | Val: 0.0585 | Test: 0.0792 | Best Val: 0.0585 (epoch 20) ðŸ”¥
2025-12-08 05:31:00,020 - GCN Epoch 030/100 | Loss: 1.2245 | Val: 0.0572 | Test: 0.0812 | Best Val: 0.0585 (epoch 20)
2025-12-08 05:31:00,597 - GCN Epoch 040/100 | Loss: 1.2092 | Val: 0.0634 | Test: 0.0987 | Best Val: 0.0634 (epoch 40) ðŸ”¥
2025-12-08 05:31:01,175 - GCN Epoch 050/100 | Loss: 1.1968 | Val: 0.1293 | Test: 0.1355 | Best Val: 0.1293 (epoch 50) ðŸ”¥
2025-12-08 05:31:01,751 - GCN Epoch 060/100 | Loss: 1.1878 | Val: 0.1334 | Test: 0.1745 | Best Val: 0.1334 (epoch 60) ðŸ”¥
2025-12-08 05:31:02,328 - GCN Epoch 070/100 | Loss: 1.1848 | Val: 0.0971 | Test: 0.1401 | Best Val: 0.1334 (epoch 60)
2025-12-08 05:31:02,904 - GCN Epoch 080/100 | Loss: 1.1846 | Val: 0.0789 | Test: 0.1023 | Best Val: 0.1334 (epoch 60)
2025-12-08 05:31:03,481 - GCN Epoch 090/100 | Loss: 1.1822 | Val: 0.1320 | Test: 0.1577 | Best Val: 0.1334 (epoch 60)
2025-12-08 05:31:04,057 - GCN Epoch 100/100 | Loss: 1.1830 | Val: 0.0579 | Test: 0.1325 | Best Val: 0.1334 (epoch 60)
2025-12-08 05:31:04,057 - GCN FINAL: Val Hits@20 = 0.1334 | Test Hits@20 = 0.1745 (at epoch 60)
2025-12-08 05:31:04,095 - 
================================================================================
2025-12-08 05:31:04,095 - Training GraphSAGE
2025-12-08 05:31:04,095 - ================================================================================
2025-12-08 05:31:04,157 - GraphSAGE Epoch 001/100 | Loss: 1.3941 | Val: 0.0016 | Test: 0.0050 | Best Val: 0.0016 (epoch 1) ðŸ”¥
2025-12-08 05:31:04,579 - GraphSAGE Epoch 010/100 | Loss: 1.2533 | Val: 0.0139 | Test: 0.0193 | Best Val: 0.0139 (epoch 10) ðŸ”¥
2025-12-08 05:31:05,045 - GraphSAGE Epoch 020/100 | Loss: 1.2344 | Val: 0.0237 | Test: 0.0570 | Best Val: 0.0237 (epoch 20) ðŸ”¥
2025-12-08 05:31:05,512 - GraphSAGE Epoch 030/100 | Loss: 1.2278 | Val: 0.0299 | Test: 0.0753 | Best Val: 0.0299 (epoch 30) ðŸ”¥
2025-12-08 05:31:05,978 - GraphSAGE Epoch 040/100 | Loss: 1.2128 | Val: 0.0243 | Test: 0.0308 | Best Val: 0.0299 (epoch 30)
2025-12-08 05:31:06,444 - GraphSAGE Epoch 050/100 | Loss: 1.1866 | Val: 0.0534 | Test: 0.0559 | Best Val: 0.0534 (epoch 50) ðŸ”¥
2025-12-08 05:31:06,911 - GraphSAGE Epoch 060/100 | Loss: 1.1713 | Val: 0.1026 | Test: 0.1256 | Best Val: 0.1026 (epoch 60) ðŸ”¥
2025-12-08 05:31:07,377 - GraphSAGE Epoch 070/100 | Loss: 1.1668 | Val: 0.0782 | Test: 0.1113 | Best Val: 0.1026 (epoch 60)
2025-12-08 05:31:07,844 - GraphSAGE Epoch 080/100 | Loss: 1.1647 | Val: 0.1043 | Test: 0.1351 | Best Val: 0.1043 (epoch 80) ðŸ”¥
2025-12-08 05:31:08,310 - GraphSAGE Epoch 090/100 | Loss: 1.1631 | Val: 0.1022 | Test: 0.1236 | Best Val: 0.1043 (epoch 80)
2025-12-08 05:31:08,776 - GraphSAGE Epoch 100/100 | Loss: 1.1535 | Val: 0.1713 | Test: 0.2152 | Best Val: 0.1713 (epoch 100) ðŸ”¥
2025-12-08 05:31:08,776 - GraphSAGE FINAL: Val Hits@20 = 0.1713 | Test Hits@20 = 0.2152 (at epoch 100)
2025-12-08 05:31:08,837 - 
================================================================================
2025-12-08 05:31:08,837 - Training GAT
2025-12-08 05:31:08,837 - ================================================================================
2025-12-08 05:31:08,937 - GAT Epoch 001/100 | Loss: 1.3863 | Val: 0.0010 | Test: 0.0007 | Best Val: 0.0010 (epoch 1) ðŸ”¥
2025-12-08 05:31:09,562 - GAT Epoch 010/100 | Loss: 1.3297 | Val: 0.0056 | Test: 0.0020 | Best Val: 0.0056 (epoch 10) ðŸ”¥
2025-12-08 05:31:10,252 - GAT Epoch 020/100 | Loss: 1.2886 | Val: 0.0075 | Test: 0.0019 | Best Val: 0.0075 (epoch 20) ðŸ”¥
2025-12-08 05:31:10,943 - GAT Epoch 030/100 | Loss: 1.2701 | Val: 0.0024 | Test: 0.0016 | Best Val: 0.0075 (epoch 20)
2025-12-08 05:31:11,634 - GAT Epoch 040/100 | Loss: 1.2578 | Val: 0.0363 | Test: 0.0109 | Best Val: 0.0363 (epoch 40) ðŸ”¥
2025-12-08 05:31:12,325 - GAT Epoch 050/100 | Loss: 1.2545 | Val: 0.0407 | Test: 0.0112 | Best Val: 0.0407 (epoch 50) ðŸ”¥
2025-12-08 05:31:13,016 - GAT Epoch 060/100 | Loss: 1.2464 | Val: 0.0393 | Test: 0.0096 | Best Val: 0.0407 (epoch 50)
2025-12-08 05:31:13,707 - GAT Epoch 070/100 | Loss: 1.2402 | Val: 0.0414 | Test: 0.0106 | Best Val: 0.0414 (epoch 70) ðŸ”¥
2025-12-08 05:31:14,398 - GAT Epoch 080/100 | Loss: 1.2379 | Val: 0.0421 | Test: 0.0122 | Best Val: 0.0421 (epoch 80) ðŸ”¥
2025-12-08 05:31:15,089 - GAT Epoch 090/100 | Loss: 1.2325 | Val: 0.0497 | Test: 0.0174 | Best Val: 0.0497 (epoch 90) ðŸ”¥
2025-12-08 05:31:15,780 - GAT Epoch 100/100 | Loss: 1.2272 | Val: 0.1099 | Test: 0.0624 | Best Val: 0.1099 (epoch 100) ðŸ”¥
2025-12-08 05:31:15,780 - GAT FINAL: Val Hits@20 = 0.1099 | Test Hits@20 = 0.0624 (at epoch 100)
2025-12-08 05:31:15,825 - 
================================================================================
2025-12-08 05:31:15,825 - Training Transformer
2025-12-08 05:31:15,826 - ================================================================================
2025-12-08 05:31:15,984 - Transformer Epoch 001/100 | Loss: 1.4536 | Val: 0.0004 | Test: 0.0002 | Best Val: 0.0004 (epoch 1) ðŸ”¥
2025-12-08 05:31:16,916 - Transformer Epoch 010/100 | Loss: 1.2396 | Val: 0.0220 | Test: 0.0301 | Best Val: 0.0220 (epoch 10) ðŸ”¥
2025-12-08 05:31:17,945 - Transformer Epoch 020/100 | Loss: 1.2339 | Val: 0.0214 | Test: 0.0500 | Best Val: 0.0220 (epoch 10)
2025-12-08 05:31:18,973 - Transformer Epoch 030/100 | Loss: 1.2293 | Val: 0.0285 | Test: 0.0600 | Best Val: 0.0285 (epoch 30) ðŸ”¥
2025-12-08 05:31:20,001 - Transformer Epoch 040/100 | Loss: 1.2198 | Val: 0.0133 | Test: 0.0125 | Best Val: 0.0285 (epoch 30)
2025-12-08 05:31:21,030 - Transformer Epoch 050/100 | Loss: 1.2105 | Val: 0.0384 | Test: 0.0753 | Best Val: 0.0384 (epoch 50) ðŸ”¥
2025-12-08 05:31:22,058 - Transformer Epoch 060/100 | Loss: 1.1999 | Val: 0.0642 | Test: 0.1373 | Best Val: 0.0642 (epoch 60) ðŸ”¥
2025-12-08 05:31:23,086 - Transformer Epoch 070/100 | Loss: 1.1977 | Val: 0.0578 | Test: 0.1510 | Best Val: 0.0642 (epoch 60)
2025-12-08 05:31:24,115 - Transformer Epoch 080/100 | Loss: 1.1978 | Val: 0.0734 | Test: 0.1604 | Best Val: 0.0734 (epoch 80) ðŸ”¥
2025-12-08 05:31:25,143 - Transformer Epoch 090/100 | Loss: 1.1971 | Val: 0.0705 | Test: 0.1409 | Best Val: 0.0734 (epoch 80)
2025-12-08 05:31:26,172 - Transformer Epoch 100/100 | Loss: 1.1964 | Val: 0.0838 | Test: 0.1581 | Best Val: 0.0838 (epoch 100) ðŸ”¥
2025-12-08 05:31:26,172 - Transformer FINAL: Val Hits@20 = 0.0838 | Test Hits@20 = 0.1581 (at epoch 100)
2025-12-08 05:31:26,172 - 
================================================================================
2025-12-08 05:31:26,172 - FINAL RESULTS COMPARISON
2025-12-08 05:31:26,172 - ================================================================================
2025-12-08 05:31:26,172 - Model           Val Hits@20     Test Hits@20   
2025-12-08 05:31:26,172 - --------------------------------------------------------------------------------
2025-12-08 05:31:26,172 - GCN             0.1334          0.1745         
2025-12-08 05:31:26,172 - GraphSAGE       0.1713          0.2152         
2025-12-08 05:31:26,172 - GAT             0.1099          0.0624         
2025-12-08 05:31:26,172 - Transformer     0.0838          0.1581         
2025-12-08 05:31:26,173 - ================================================================================
2025-12-08 05:31:26,173 - 
Best Model: GraphSAGE with 0.1713 Val Hits@20
2025-12-08 05:31:26,173 - 
================================================================================
2025-12-08 05:31:26,173 - ANALYSIS
2025-12-08 05:31:26,173 - ================================================================================
2025-12-08 05:31:26,173 - âš ï¸  MIXED RESULTS: Some models below 20%
2025-12-08 05:31:26,173 -    â†’ Poor performers: GCN, GraphSAGE, GAT, Transformer
2025-12-08 05:31:26,173 -    â†’ Focus on models that work well (>20%)
2025-12-08 05:31:26,173 - 
Recommended Next Steps:
2025-12-08 05:31:26,173 - 1. Take GraphSAGE as base architecture (best performer)
2025-12-08 05:31:26,173 - 2. Add improvements incrementally: BatchNorm â†’ Dropout â†’ 3 layers â†’ MLP decoder
2025-12-08 05:31:26,173 - 3. Test each addition independently
2025-12-08 05:31:26,173 - 4. Only keep changes that improve validation Hits@20
2025-12-08 05:31:26,173 - 
Log saved to: logs/minimal_baselines_20251208_053057/minimal_baselines.log
2025-12-08 05:31:26,173 - ================================================================================
