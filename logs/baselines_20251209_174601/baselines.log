2025-12-09 17:46:01,733 - Using device: cuda
2025-12-09 17:46:01,733 - Logging results to: logs/baselines_20251209_174601/baselines.log
2025-12-09 17:46:01,733 - Loading dataset ogbl-ddi...
2025-12-09 17:46:01,818 - Dataset loaded: 4267 nodes
2025-12-09 17:46:01,962 - Train pos edges: 1067911, Valid pos: 133489, Test pos: 133489
2025-12-09 17:46:01,962 - Valid neg edges: 101882, Test neg: 95599
2025-12-09 17:46:01,982 - Added self-loops: Total edges now = 1072178
2025-12-09 17:46:01,989 - 
================================================================================
2025-12-09 17:46:01,989 - Training Simple GCN Baseline (minimal trainer)
2025-12-09 17:46:01,989 - ================================================================================
2025-12-09 17:46:02,133 - [GCN-Baseline] Starting minimal baseline training (epochs=200, lr=0.01, wd=0.0001, batch_size=50000, eval_every=5)
2025-12-09 17:46:03,927 - [GCN-Baseline] Epoch 0001 | loss 1.3881 | val@20 0.0111 | test@20 0.0035 | best 0.0111 (ep 1)
2025-12-09 17:46:09,434 - [GCN-Baseline] Epoch 0005 | loss 1.9631 | val@20 0.0189 | test@20 0.0378 | best 0.0189 (ep 5)
2025-12-09 17:46:16,182 - [GCN-Baseline] Epoch 0010 | loss 1.3758 | val@20 0.0160 | test@20 0.0018 | best 0.0189 (ep 5)
2025-12-09 17:46:22,919 - [GCN-Baseline] Epoch 0015 | loss 1.3313 | val@20 0.0145 | test@20 0.0100 | best 0.0189 (ep 5)
2025-12-09 17:46:29,661 - [GCN-Baseline] Epoch 0020 | loss 1.4911 | val@20 0.0467 | test@20 0.0257 | best 0.0467 (ep 20)
2025-12-09 17:46:36,417 - [GCN-Baseline] Epoch 0025 | loss 1.1745 | val@20 0.0386 | test@20 0.0108 | best 0.0467 (ep 20)
2025-12-09 17:46:43,160 - [GCN-Baseline] Epoch 0030 | loss 1.2180 | val@20 0.0552 | test@20 0.0516 | best 0.0552 (ep 30)
2025-12-09 17:46:49,868 - [GCN-Baseline] Epoch 0035 | loss 1.0742 | val@20 0.0267 | test@20 0.0469 | best 0.0552 (ep 30)
2025-12-09 17:46:56,556 - [GCN-Baseline] Epoch 0040 | loss 1.0240 | val@20 0.0560 | test@20 0.0545 | best 0.0560 (ep 40)
2025-12-09 17:47:03,201 - [GCN-Baseline] Epoch 0045 | loss 0.9513 | val@20 0.0350 | test@20 0.0560 | best 0.0560 (ep 40)
2025-12-09 17:47:09,837 - [GCN-Baseline] Epoch 0050 | loss 0.9236 | val@20 0.0506 | test@20 0.0548 | best 0.0560 (ep 40)
2025-12-09 17:47:16,454 - [GCN-Baseline] Epoch 0055 | loss 0.9096 | val@20 0.0526 | test@20 0.0588 | best 0.0560 (ep 40)
2025-12-09 17:47:23,030 - [GCN-Baseline] Epoch 0060 | loss 0.9073 | val@20 0.0535 | test@20 0.0604 | best 0.0560 (ep 40)
2025-12-09 17:47:29,636 - [GCN-Baseline] Epoch 0065 | loss 0.8751 | val@20 0.0516 | test@20 0.0569 | best 0.0560 (ep 40)
2025-12-09 17:47:36,211 - [GCN-Baseline] Epoch 0070 | loss 0.8662 | val@20 0.0426 | test@20 0.0542 | best 0.0560 (ep 40)
2025-12-09 17:47:42,766 - [GCN-Baseline] Epoch 0075 | loss 0.8567 | val@20 0.0430 | test@20 0.0476 | best 0.0560 (ep 40)
2025-12-09 17:47:49,312 - [GCN-Baseline] Epoch 0080 | loss 0.8422 | val@20 0.0328 | test@20 0.0474 | best 0.0560 (ep 40)
2025-12-09 17:47:55,866 - [GCN-Baseline] Epoch 0085 | loss 0.8550 | val@20 0.0132 | test@20 0.0211 | best 0.0560 (ep 40)
2025-12-09 17:48:02,437 - [GCN-Baseline] Epoch 0090 | loss 0.8253 | val@20 0.0307 | test@20 0.0085 | best 0.0560 (ep 40)
2025-12-09 17:48:08,988 - [GCN-Baseline] Epoch 0095 | loss 0.7964 | val@20 0.0481 | test@20 0.0269 | best 0.0560 (ep 40)
2025-12-09 17:48:15,564 - [GCN-Baseline] Epoch 0100 | loss 0.8066 | val@20 0.0022 | test@20 0.0003 | best 0.0560 (ep 40)
2025-12-09 17:48:22,177 - [GCN-Baseline] Epoch 0105 | loss 0.7844 | val@20 0.0855 | test@20 0.1169 | best 0.0855 (ep 105)
2025-12-09 17:48:28,755 - [GCN-Baseline] Epoch 0110 | loss 0.7681 | val@20 0.1041 | test@20 0.1141 | best 0.1041 (ep 110)
2025-12-09 17:48:35,341 - [GCN-Baseline] Epoch 0115 | loss 0.7610 | val@20 0.0035 | test@20 0.0008 | best 0.1041 (ep 110)
2025-12-09 17:48:41,914 - [GCN-Baseline] Epoch 0120 | loss 0.7570 | val@20 0.0251 | test@20 0.0196 | best 0.1041 (ep 110)
2025-12-09 17:48:48,495 - [GCN-Baseline] Epoch 0125 | loss 0.7501 | val@20 0.0190 | test@20 0.0205 | best 0.1041 (ep 110)
2025-12-09 17:48:55,070 - [GCN-Baseline] Epoch 0130 | loss 0.7426 | val@20 0.0149 | test@20 0.0043 | best 0.1041 (ep 110)
2025-12-09 17:49:01,696 - [GCN-Baseline] Epoch 0135 | loss 0.7397 | val@20 0.0221 | test@20 0.0483 | best 0.1041 (ep 110)
2025-12-09 17:49:08,266 - [GCN-Baseline] Epoch 0140 | loss 0.7347 | val@20 0.0354 | test@20 0.0678 | best 0.1041 (ep 110)
2025-12-09 17:49:14,874 - [GCN-Baseline] Epoch 0145 | loss 0.7319 | val@20 0.0223 | test@20 0.0675 | best 0.1041 (ep 110)
2025-12-09 17:49:21,421 - [GCN-Baseline] Epoch 0150 | loss 0.7276 | val@20 0.0282 | test@20 0.0831 | best 0.1041 (ep 110)
2025-12-09 17:49:28,050 - [GCN-Baseline] Epoch 0155 | loss 0.7209 | val@20 0.0232 | test@20 0.0634 | best 0.1041 (ep 110)
2025-12-09 17:49:34,636 - [GCN-Baseline] Epoch 0160 | loss 0.7541 | val@20 0.0431 | test@20 0.1220 | best 0.1041 (ep 110)
2025-12-09 17:49:41,229 - [GCN-Baseline] Epoch 0165 | loss 0.7159 | val@20 0.0281 | test@20 0.0615 | best 0.1041 (ep 110)
2025-12-09 17:49:47,848 - [GCN-Baseline] Epoch 0170 | loss 0.7086 | val@20 0.0498 | test@20 0.0605 | best 0.1041 (ep 110)
2025-12-09 17:49:54,402 - [GCN-Baseline] Epoch 0175 | loss 0.7062 | val@20 0.0498 | test@20 0.1038 | best 0.1041 (ep 110)
2025-12-09 17:50:00,995 - [GCN-Baseline] Epoch 0180 | loss 0.7053 | val@20 0.1118 | test@20 0.0995 | best 0.1118 (ep 180)
2025-12-09 17:50:07,573 - [GCN-Baseline] Epoch 0185 | loss 0.7550 | val@20 0.0698 | test@20 0.0998 | best 0.1118 (ep 180)
2025-12-09 17:50:14,170 - [GCN-Baseline] Epoch 0190 | loss 0.7328 | val@20 0.0996 | test@20 0.1172 | best 0.1118 (ep 180)
2025-12-09 17:50:20,752 - [GCN-Baseline] Epoch 0195 | loss 0.7028 | val@20 0.0621 | test@20 0.0996 | best 0.1118 (ep 180)
2025-12-09 17:50:27,349 - [GCN-Baseline] Epoch 0200 | loss 0.6877 | val@20 0.0777 | test@20 0.1148 | best 0.1118 (ep 180)
2025-12-09 17:50:27,349 - [GCN-Baseline] Done. Best val@20=0.1118 | test@20=0.0995 (epoch 180)
2025-12-09 17:50:27,349 - 
================================================================================
2025-12-09 17:50:27,349 - Training GraphSAGE Baseline (minimal trainer)
2025-12-09 17:50:27,349 - ================================================================================
2025-12-09 17:50:27,385 - [GraphSAGE-Baseline] Starting minimal baseline training (epochs=200, lr=0.01, wd=0.0001, batch_size=50000, eval_every=5)
2025-12-09 17:50:28,828 - [GraphSAGE-Baseline] Epoch 0001 | loss 1.3891 | val@20 0.0011 | test@20 0.0021 | best 0.0011 (ep 1)
2025-12-09 17:50:34,149 - [GraphSAGE-Baseline] Epoch 0005 | loss 1.2809 | val@20 0.0207 | test@20 0.0219 | best 0.0207 (ep 5)
2025-12-09 17:50:40,722 - [GraphSAGE-Baseline] Epoch 0010 | loss 1.1356 | val@20 0.0158 | test@20 0.0227 | best 0.0207 (ep 5)
2025-12-09 17:50:47,373 - [GraphSAGE-Baseline] Epoch 0015 | loss 1.0032 | val@20 0.0256 | test@20 0.0510 | best 0.0256 (ep 15)
2025-12-09 17:50:53,978 - [GraphSAGE-Baseline] Epoch 0020 | loss 0.9620 | val@20 0.0238 | test@20 0.0319 | best 0.0256 (ep 15)
2025-12-09 17:51:00,527 - [GraphSAGE-Baseline] Epoch 0025 | loss 0.9269 | val@20 0.0255 | test@20 0.0392 | best 0.0256 (ep 15)
2025-12-09 17:51:07,155 - [GraphSAGE-Baseline] Epoch 0030 | loss 0.9033 | val@20 0.0260 | test@20 0.0336 | best 0.0260 (ep 30)
2025-12-09 17:51:13,749 - [GraphSAGE-Baseline] Epoch 0035 | loss 0.8868 | val@20 0.0312 | test@20 0.0457 | best 0.0312 (ep 35)
2025-12-09 17:51:20,325 - [GraphSAGE-Baseline] Epoch 0040 | loss 0.8896 | val@20 0.0270 | test@20 0.0439 | best 0.0312 (ep 35)
2025-12-09 17:51:26,899 - [GraphSAGE-Baseline] Epoch 0045 | loss 0.8507 | val@20 0.0271 | test@20 0.0367 | best 0.0312 (ep 35)
2025-12-09 17:51:33,509 - [GraphSAGE-Baseline] Epoch 0050 | loss 0.8234 | val@20 0.0275 | test@20 0.0196 | best 0.0312 (ep 35)
2025-12-09 17:51:40,089 - [GraphSAGE-Baseline] Epoch 0055 | loss 0.8035 | val@20 0.0520 | test@20 0.0142 | best 0.0520 (ep 55)
2025-12-09 17:51:46,651 - [GraphSAGE-Baseline] Epoch 0060 | loss 0.7795 | val@20 0.0265 | test@20 0.0411 | best 0.0520 (ep 55)
2025-12-09 17:51:53,228 - [GraphSAGE-Baseline] Epoch 0065 | loss 0.7752 | val@20 0.0336 | test@20 0.0063 | best 0.0520 (ep 55)
2025-12-09 17:51:59,756 - [GraphSAGE-Baseline] Epoch 0070 | loss 0.7555 | val@20 0.0237 | test@20 0.0294 | best 0.0520 (ep 55)
2025-12-09 17:52:06,273 - [GraphSAGE-Baseline] Epoch 0075 | loss 0.7599 | val@20 0.0456 | test@20 0.0351 | best 0.0520 (ep 55)
2025-12-09 17:52:12,933 - [GraphSAGE-Baseline] Epoch 0080 | loss 0.7137 | val@20 0.0499 | test@20 0.0387 | best 0.0520 (ep 55)
2025-12-09 17:52:19,478 - [GraphSAGE-Baseline] Epoch 0085 | loss 0.7025 | val@20 0.0807 | test@20 0.0387 | best 0.0807 (ep 85)
2025-12-09 17:52:26,077 - [GraphSAGE-Baseline] Epoch 0090 | loss 0.6918 | val@20 0.0700 | test@20 0.0484 | best 0.0807 (ep 85)
2025-12-09 17:52:32,637 - [GraphSAGE-Baseline] Epoch 0095 | loss 0.6860 | val@20 0.1007 | test@20 0.0578 | best 0.1007 (ep 95)
2025-12-09 17:52:39,167 - [GraphSAGE-Baseline] Epoch 0100 | loss 0.6793 | val@20 0.0891 | test@20 0.0779 | best 0.1007 (ep 95)
2025-12-09 17:52:45,730 - [GraphSAGE-Baseline] Epoch 0105 | loss 0.6622 | val@20 0.0449 | test@20 0.0452 | best 0.1007 (ep 95)
2025-12-09 17:52:52,293 - [GraphSAGE-Baseline] Epoch 0110 | loss 0.6507 | val@20 0.0666 | test@20 0.0581 | best 0.1007 (ep 95)
2025-12-09 17:52:58,838 - [GraphSAGE-Baseline] Epoch 0115 | loss 0.6415 | val@20 0.0750 | test@20 0.0603 | best 0.1007 (ep 95)
2025-12-09 17:53:05,338 - [GraphSAGE-Baseline] Epoch 0120 | loss 0.6533 | val@20 0.0490 | test@20 0.0069 | best 0.1007 (ep 95)
2025-12-09 17:53:11,978 - [GraphSAGE-Baseline] Epoch 0125 | loss 0.6392 | val@20 0.1061 | test@20 0.0646 | best 0.1061 (ep 125)
2025-12-09 17:53:18,514 - [GraphSAGE-Baseline] Epoch 0130 | loss 0.6259 | val@20 0.1072 | test@20 0.0751 | best 0.1072 (ep 130)
2025-12-09 17:53:25,097 - [GraphSAGE-Baseline] Epoch 0135 | loss 0.6365 | val@20 0.0582 | test@20 0.0356 | best 0.1072 (ep 130)
2025-12-09 17:53:31,676 - [GraphSAGE-Baseline] Epoch 0140 | loss 0.6267 | val@20 0.0600 | test@20 0.0086 | best 0.1072 (ep 130)
2025-12-09 17:53:38,172 - [GraphSAGE-Baseline] Epoch 0145 | loss 0.6102 | val@20 0.1244 | test@20 0.0729 | best 0.1244 (ep 145)
2025-12-09 17:53:44,712 - [GraphSAGE-Baseline] Epoch 0150 | loss 0.6364 | val@20 0.0920 | test@20 0.0237 | best 0.1244 (ep 145)
2025-12-09 17:53:51,260 - [GraphSAGE-Baseline] Epoch 0155 | loss 0.6165 | val@20 0.0851 | test@20 0.0587 | best 0.1244 (ep 145)
2025-12-09 17:53:57,867 - [GraphSAGE-Baseline] Epoch 0160 | loss 0.6002 | val@20 0.0633 | test@20 0.0097 | best 0.1244 (ep 145)
2025-12-09 17:54:04,438 - [GraphSAGE-Baseline] Epoch 0165 | loss 0.5867 | val@20 0.0795 | test@20 0.0249 | best 0.1244 (ep 145)
2025-12-09 17:54:11,067 - [GraphSAGE-Baseline] Epoch 0170 | loss 0.5758 | val@20 0.0282 | test@20 0.0073 | best 0.1244 (ep 145)
2025-12-09 17:54:17,639 - [GraphSAGE-Baseline] Epoch 0175 | loss 0.5664 | val@20 0.0318 | test@20 0.0221 | best 0.1244 (ep 145)
2025-12-09 17:54:24,259 - [GraphSAGE-Baseline] Epoch 0180 | loss 0.5567 | val@20 0.0621 | test@20 0.0288 | best 0.1244 (ep 145)
2025-12-09 17:54:30,832 - [GraphSAGE-Baseline] Epoch 0185 | loss 0.5460 | val@20 0.1045 | test@20 0.0626 | best 0.1244 (ep 145)
2025-12-09 17:54:37,362 - [GraphSAGE-Baseline] Epoch 0190 | loss 0.5489 | val@20 0.0184 | test@20 0.0044 | best 0.1244 (ep 145)
2025-12-09 17:54:43,886 - [GraphSAGE-Baseline] Epoch 0195 | loss 0.5666 | val@20 0.0279 | test@20 0.0028 | best 0.1244 (ep 145)
2025-12-09 17:54:50,443 - [GraphSAGE-Baseline] Epoch 0200 | loss 0.5478 | val@20 0.1293 | test@20 0.0594 | best 0.1293 (ep 200)
2025-12-09 17:54:50,443 - [GraphSAGE-Baseline] Done. Best val@20=0.1293 | test@20=0.0594 (epoch 200)
2025-12-09 17:54:50,443 - 
================================================================================
2025-12-09 17:54:50,444 - Training GraphTransformer Baseline (minimal trainer)
2025-12-09 17:54:50,444 - ================================================================================
2025-12-09 17:54:50,483 - [GraphTransformer-Baseline] Starting minimal baseline training (epochs=200, lr=0.005, wd=0.0001, batch_size=50000, eval_every=5)
2025-12-09 17:54:52,022 - [GraphTransformer-Baseline] Epoch 0001 | loss 1.3863 | val@20 0.0012 | test@20 0.0017 | best 0.0012 (ep 1)
2025-12-09 17:54:57,597 - [GraphTransformer-Baseline] Epoch 0005 | loss 1.3791 | val@20 0.0122 | test@20 0.0075 | best 0.0122 (ep 5)
2025-12-09 17:55:04,510 - [GraphTransformer-Baseline] Epoch 0010 | loss 1.1756 | val@20 0.0222 | test@20 0.0149 | best 0.0222 (ep 10)
2025-12-09 17:55:11,389 - [GraphTransformer-Baseline] Epoch 0015 | loss 0.9723 | val@20 0.0195 | test@20 0.0263 | best 0.0222 (ep 10)
2025-12-09 17:55:18,277 - [GraphTransformer-Baseline] Epoch 0020 | loss 0.9492 | val@20 0.0131 | test@20 0.0158 | best 0.0222 (ep 10)
2025-12-09 17:55:25,144 - [GraphTransformer-Baseline] Epoch 0025 | loss 0.9225 | val@20 0.0143 | test@20 0.0187 | best 0.0222 (ep 10)
2025-12-09 17:55:32,037 - [GraphTransformer-Baseline] Epoch 0030 | loss 0.8992 | val@20 0.0196 | test@20 0.0205 | best 0.0222 (ep 10)
2025-12-09 17:55:38,871 - [GraphTransformer-Baseline] Epoch 0035 | loss 0.8839 | val@20 0.0202 | test@20 0.0204 | best 0.0222 (ep 10)
2025-12-09 17:55:45,741 - [GraphTransformer-Baseline] Epoch 0040 | loss 0.8576 | val@20 0.0208 | test@20 0.0237 | best 0.0222 (ep 10)
2025-12-09 17:55:52,596 - [GraphTransformer-Baseline] Epoch 0045 | loss 0.8241 | val@20 0.0093 | test@20 0.0076 | best 0.0222 (ep 10)
2025-12-09 17:55:59,486 - [GraphTransformer-Baseline] Epoch 0050 | loss 0.8042 | val@20 0.0175 | test@20 0.0276 | best 0.0222 (ep 10)
2025-12-09 17:56:06,360 - [GraphTransformer-Baseline] Epoch 0055 | loss 0.8128 | val@20 0.0520 | test@20 0.0355 | best 0.0520 (ep 55)
2025-12-09 17:56:13,210 - [GraphTransformer-Baseline] Epoch 0060 | loss 0.7965 | val@20 0.0450 | test@20 0.0254 | best 0.0520 (ep 55)
2025-12-09 17:56:20,100 - [GraphTransformer-Baseline] Epoch 0065 | loss 0.7761 | val@20 0.0307 | test@20 0.0377 | best 0.0520 (ep 55)
2025-12-09 17:56:26,974 - [GraphTransformer-Baseline] Epoch 0070 | loss 0.7624 | val@20 0.0330 | test@20 0.0372 | best 0.0520 (ep 55)
2025-12-09 17:56:33,854 - [GraphTransformer-Baseline] Epoch 0075 | loss 0.7561 | val@20 0.0342 | test@20 0.0368 | best 0.0520 (ep 55)
2025-12-09 17:56:40,723 - [GraphTransformer-Baseline] Epoch 0080 | loss 0.7344 | val@20 0.0223 | test@20 0.0220 | best 0.0520 (ep 55)
2025-12-09 17:56:47,584 - [GraphTransformer-Baseline] Epoch 0085 | loss 0.7162 | val@20 0.0298 | test@20 0.0265 | best 0.0520 (ep 55)
2025-12-09 17:56:54,439 - [GraphTransformer-Baseline] Epoch 0090 | loss 0.7123 | val@20 0.0383 | test@20 0.0236 | best 0.0520 (ep 55)
2025-12-09 17:57:01,308 - [GraphTransformer-Baseline] Epoch 0095 | loss 0.7155 | val@20 0.0580 | test@20 0.0381 | best 0.0580 (ep 95)
2025-12-09 17:57:08,226 - [GraphTransformer-Baseline] Epoch 0100 | loss 0.7086 | val@20 0.0986 | test@20 0.0412 | best 0.0986 (ep 100)
2025-12-09 17:57:15,119 - [GraphTransformer-Baseline] Epoch 0105 | loss 0.6913 | val@20 0.1283 | test@20 0.0590 | best 0.1283 (ep 105)
2025-12-09 17:57:22,012 - [GraphTransformer-Baseline] Epoch 0110 | loss 0.6838 | val@20 0.0925 | test@20 0.0608 | best 0.1283 (ep 105)
2025-12-09 17:57:28,905 - [GraphTransformer-Baseline] Epoch 0115 | loss 0.6783 | val@20 0.0666 | test@20 0.0480 | best 0.1283 (ep 105)
2025-12-09 17:57:35,791 - [GraphTransformer-Baseline] Epoch 0120 | loss 0.6731 | val@20 0.0678 | test@20 0.0520 | best 0.1283 (ep 105)
2025-12-09 17:57:42,733 - [GraphTransformer-Baseline] Epoch 0125 | loss 0.6678 | val@20 0.0533 | test@20 0.0485 | best 0.1283 (ep 105)
2025-12-09 17:57:49,626 - [GraphTransformer-Baseline] Epoch 0130 | loss 0.6603 | val@20 0.0819 | test@20 0.0368 | best 0.1283 (ep 105)
2025-12-09 17:57:56,527 - [GraphTransformer-Baseline] Epoch 0135 | loss 0.6550 | val@20 0.0627 | test@20 0.0416 | best 0.1283 (ep 105)
2025-12-09 17:58:03,407 - [GraphTransformer-Baseline] Epoch 0140 | loss 0.6651 | val@20 0.0689 | test@20 0.0491 | best 0.1283 (ep 105)
2025-12-09 17:58:10,285 - [GraphTransformer-Baseline] Epoch 0145 | loss 0.6454 | val@20 0.0656 | test@20 0.0472 | best 0.1283 (ep 105)
2025-12-09 17:58:17,198 - [GraphTransformer-Baseline] Epoch 0150 | loss 0.7059 | val@20 0.0459 | test@20 0.0458 | best 0.1283 (ep 105)
2025-12-09 17:58:24,067 - [GraphTransformer-Baseline] Epoch 0155 | loss 0.6679 | val@20 0.0813 | test@20 0.0898 | best 0.1283 (ep 105)
2025-12-09 17:58:30,976 - [GraphTransformer-Baseline] Epoch 0160 | loss 0.6700 | val@20 0.0939 | test@20 0.0949 | best 0.1283 (ep 105)
2025-12-09 17:58:37,860 - [GraphTransformer-Baseline] Epoch 0165 | loss 0.6707 | val@20 0.0722 | test@20 0.0761 | best 0.1283 (ep 105)
2025-12-09 17:58:44,728 - [GraphTransformer-Baseline] Epoch 0170 | loss 0.6528 | val@20 0.0658 | test@20 0.0552 | best 0.1283 (ep 105)
2025-12-09 17:58:51,603 - [GraphTransformer-Baseline] Epoch 0175 | loss 0.6362 | val@20 0.1039 | test@20 0.0749 | best 0.1283 (ep 105)
2025-12-09 17:58:58,506 - [GraphTransformer-Baseline] Epoch 0180 | loss 0.6208 | val@20 0.1114 | test@20 0.0671 | best 0.1283 (ep 105)
2025-12-09 17:59:05,405 - [GraphTransformer-Baseline] Epoch 0185 | loss 0.6114 | val@20 0.0845 | test@20 0.0520 | best 0.1283 (ep 105)
2025-12-09 17:59:12,320 - [GraphTransformer-Baseline] Epoch 0190 | loss 0.6034 | val@20 0.0865 | test@20 0.0506 | best 0.1283 (ep 105)
2025-12-09 17:59:19,265 - [GraphTransformer-Baseline] Epoch 0195 | loss 0.5970 | val@20 0.0906 | test@20 0.0686 | best 0.1283 (ep 105)
2025-12-09 17:59:26,225 - [GraphTransformer-Baseline] Epoch 0200 | loss 0.5902 | val@20 0.0862 | test@20 0.0761 | best 0.1283 (ep 105)
2025-12-09 17:59:26,226 - [GraphTransformer-Baseline] Done. Best val@20=0.1283 | test@20=0.0590 (epoch 105)
2025-12-09 17:59:26,226 - 
================================================================================
2025-12-09 17:59:26,226 - Training GAT Baseline (minimal trainer)
2025-12-09 17:59:26,226 - ================================================================================
2025-12-09 17:59:26,285 - [GAT-Baseline] Starting minimal baseline training (epochs=200, lr=0.005, wd=0.0001, batch_size=50000, eval_every=5)
2025-12-09 17:59:27,775 - [GAT-Baseline] Epoch 0001 | loss 1.3864 | val@20 0.0013 | test@20 0.0004 | best 0.0013 (ep 1)
2025-12-09 17:59:33,138 - [GAT-Baseline] Epoch 0005 | loss 1.3859 | val@20 0.0028 | test@20 0.0008 | best 0.0028 (ep 5)
2025-12-09 17:59:39,806 - [GAT-Baseline] Epoch 0010 | loss 1.3654 | val@20 0.0012 | test@20 0.0004 | best 0.0028 (ep 5)
2025-12-09 17:59:46,536 - [GAT-Baseline] Epoch 0015 | loss 1.2861 | val@20 0.0006 | test@20 0.0004 | best 0.0028 (ep 5)
2025-12-09 17:59:53,420 - [GAT-Baseline] Epoch 0020 | loss 1.2232 | val@20 0.0163 | test@20 0.0018 | best 0.0163 (ep 20)
2025-12-09 18:00:00,643 - [GAT-Baseline] Epoch 0025 | loss 1.1762 | val@20 0.0015 | test@20 0.0008 | best 0.0163 (ep 20)
2025-12-09 18:00:07,604 - [GAT-Baseline] Epoch 0030 | loss 1.0899 | val@20 0.0036 | test@20 0.0012 | best 0.0163 (ep 20)
2025-12-09 18:00:14,807 - [GAT-Baseline] Epoch 0035 | loss 1.0379 | val@20 0.0017 | test@20 0.0006 | best 0.0163 (ep 20)
2025-12-09 18:00:21,839 - [GAT-Baseline] Epoch 0040 | loss 0.9869 | val@20 0.0075 | test@20 0.0025 | best 0.0163 (ep 20)
2025-12-09 18:00:30,186 - [GAT-Baseline] Epoch 0045 | loss 0.9617 | val@20 0.0080 | test@20 0.0020 | best 0.0163 (ep 20)
2025-12-09 18:00:38,470 - [GAT-Baseline] Epoch 0050 | loss 0.9289 | val@20 0.0042 | test@20 0.0020 | best 0.0163 (ep 20)
2025-12-09 18:00:45,932 - [GAT-Baseline] Epoch 0055 | loss 0.9148 | val@20 0.0497 | test@20 0.0320 | best 0.0497 (ep 55)
2025-12-09 18:00:52,693 - [GAT-Baseline] Epoch 0060 | loss 0.9016 | val@20 0.1142 | test@20 0.0531 | best 0.1142 (ep 60)
2025-12-09 18:00:59,474 - [GAT-Baseline] Epoch 0065 | loss 0.8896 | val@20 0.1190 | test@20 0.0728 | best 0.1190 (ep 65)
2025-12-09 18:01:06,202 - [GAT-Baseline] Epoch 0070 | loss 0.8776 | val@20 0.1050 | test@20 0.0775 | best 0.1190 (ep 65)
2025-12-09 18:01:12,932 - [GAT-Baseline] Epoch 0075 | loss 0.8701 | val@20 0.1057 | test@20 0.0751 | best 0.1190 (ep 65)
2025-12-09 18:01:22,477 - [GAT-Baseline] Epoch 0080 | loss 0.8576 | val@20 0.0927 | test@20 0.0609 | best 0.1190 (ep 65)
2025-12-09 18:01:30,017 - [GAT-Baseline] Epoch 0085 | loss 0.9800 | val@20 0.0071 | test@20 0.0017 | best 0.1190 (ep 65)
2025-12-09 18:01:36,835 - [GAT-Baseline] Epoch 0090 | loss 0.8727 | val@20 0.0983 | test@20 0.0626 | best 0.1190 (ep 65)
2025-12-09 18:01:43,548 - [GAT-Baseline] Epoch 0095 | loss 0.8812 | val@20 0.1149 | test@20 0.0946 | best 0.1190 (ep 65)
2025-12-09 18:01:50,289 - [GAT-Baseline] Epoch 0100 | loss 0.8702 | val@20 0.0698 | test@20 0.0568 | best 0.1190 (ep 65)
2025-12-09 18:01:57,065 - [GAT-Baseline] Epoch 0105 | loss 0.8493 | val@20 0.0841 | test@20 0.0732 | best 0.1190 (ep 65)
2025-12-09 18:02:03,794 - [GAT-Baseline] Epoch 0110 | loss 0.8346 | val@20 0.0441 | test@20 0.0527 | best 0.1190 (ep 65)
2025-12-09 18:02:11,514 - [GAT-Baseline] Epoch 0115 | loss 0.8237 | val@20 0.0648 | test@20 0.0528 | best 0.1190 (ep 65)
2025-12-09 18:02:18,284 - [GAT-Baseline] Epoch 0120 | loss 0.8111 | val@20 0.0724 | test@20 0.0571 | best 0.1190 (ep 65)
2025-12-09 18:02:24,990 - [GAT-Baseline] Epoch 0125 | loss 0.7974 | val@20 0.0566 | test@20 0.0598 | best 0.1190 (ep 65)
2025-12-09 18:02:31,763 - [GAT-Baseline] Epoch 0130 | loss 0.7915 | val@20 0.0396 | test@20 0.0527 | best 0.1190 (ep 65)
2025-12-09 18:02:38,457 - [GAT-Baseline] Epoch 0135 | loss 0.7841 | val@20 0.0351 | test@20 0.0540 | best 0.1190 (ep 65)
2025-12-09 18:02:45,148 - [GAT-Baseline] Epoch 0140 | loss 0.7810 | val@20 0.0421 | test@20 0.0445 | best 0.1190 (ep 65)
2025-12-09 18:02:51,882 - [GAT-Baseline] Epoch 0145 | loss 0.7749 | val@20 0.0386 | test@20 0.0643 | best 0.1190 (ep 65)
2025-12-09 18:02:58,561 - [GAT-Baseline] Epoch 0150 | loss 0.7703 | val@20 0.0503 | test@20 0.0449 | best 0.1190 (ep 65)
2025-12-09 18:03:05,298 - [GAT-Baseline] Epoch 0155 | loss 0.7605 | val@20 0.0484 | test@20 0.0464 | best 0.1190 (ep 65)
2025-12-09 18:03:12,081 - [GAT-Baseline] Epoch 0160 | loss 0.7706 | val@20 0.0559 | test@20 0.0704 | best 0.1190 (ep 65)
2025-12-09 18:03:18,859 - [GAT-Baseline] Epoch 0165 | loss 0.7503 | val@20 0.0450 | test@20 0.0470 | best 0.1190 (ep 65)
2025-12-09 18:03:18,859 - [GAT-Baseline] Early stopping at epoch 165 (no val improvement for 20 evals)
2025-12-09 18:03:18,859 - [GAT-Baseline] Done. Best val@20=0.1190 | test@20=0.0728 (epoch 65)
2025-12-09 18:03:18,859 - 
================================================================================
2025-12-09 18:03:18,860 - FINAL RESULTS - BASELINES
2025-12-09 18:03:18,860 - ================================================================================
2025-12-09 18:03:18,860 - GCN:
2025-12-09 18:03:18,860 -   Validation Hits@20: 0.1118
2025-12-09 18:03:18,860 -   Test Hits@20: 0.0995
2025-12-09 18:03:18,860 -   Val-Test Gap: 0.0123 (11.0% relative)
2025-12-09 18:03:18,860 - GraphSAGE:
2025-12-09 18:03:18,860 -   Validation Hits@20: 0.1293
2025-12-09 18:03:18,860 -   Test Hits@20: 0.0594
2025-12-09 18:03:18,860 -   Val-Test Gap: 0.0700 (54.1% relative)
2025-12-09 18:03:18,860 - GraphTransformer:
2025-12-09 18:03:18,860 -   Validation Hits@20: 0.1283
2025-12-09 18:03:18,860 -   Test Hits@20: 0.0590
2025-12-09 18:03:18,860 -   Val-Test Gap: 0.0693 (54.0% relative)
2025-12-09 18:03:18,861 - GAT:
2025-12-09 18:03:18,861 -   Validation Hits@20: 0.1190
2025-12-09 18:03:18,861 -   Test Hits@20: 0.0728
2025-12-09 18:03:18,861 -   Val-Test Gap: 0.0462 (38.9% relative)
2025-12-09 18:03:18,861 - ================================================================================
2025-12-09 18:03:18,861 - Results logged to: logs/baselines_20251209_174601/baselines.log
