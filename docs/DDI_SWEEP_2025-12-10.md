# DDI Sweep (2025-12-10)

Sweep location: `logs/ddi_sweeps_20251210_200632` (GCN encoder, `run_ddi_sweeps.py` defaults; 100 epochs, eval every 5, single seed, device=CUDA if available). Dataset snapshot: 4,267 nodes, 2,135,822 edges.

Primary metric: Hits@20 on the OGBL-DDI test split.

## Top configs by Hits@20 (test)

| Config | Layers | Hidden | Dropout | LR | Batch | Val@20 | Test@20 | Log |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_l2_h256_do0.25_lr0.005_bs64k | 2 | 256 | 0.25 | 0.005 | 65,536 | 0.3860 | **0.3268** | `logs/ddi_sweeps_20251210_200632/A_l2_h256_do0.25_lr0.005_bs64k/config.log` |
| B_l3_h192_do0.0_lr0.001_bs32k | 3 | 192 | 0.0 | 0.001 | 32,768 | 0.2761 | **0.3066** | `logs/ddi_sweeps_20251210_200632/B_l3_h192_do0.0_lr0.001_bs32k/config.log` |
| A_l2_h192_do0.25_lr0.005_bs64k | 2 | 192 | 0.25 | 0.005 | 65,536 | 0.3936 | 0.2779 | `logs/ddi_sweeps_20251210_200632/A_l2_h192_do0.25_lr0.005_bs64k/config.log` |
| A_l2_h128_do0.1_lr0.005_bs64k | 2 | 128 | 0.1 | 0.005 | 65,536 | 0.3123 | 0.2744 | `logs/ddi_sweeps_20251210_200632/A_l2_h128_do0.1_lr0.005_bs64k/config.log` |
| A_l2_h192_do0.1_lr0.005_bs64k | 2 | 192 | 0.1 | 0.005 | 65,536 | 0.3607 | 0.2689 | `logs/ddi_sweeps_20251210_200632/A_l2_h192_do0.1_lr0.005_bs64k/config.log` |

Best zero-dropout option is `B_l3_h192_do0.0_lr0.001_bs32k` (Test@20=0.3066), second overall.

## Observations

- Depth/width: Larger hidden widths helped across families; 3-layer/192-d (dropout=0) nearly matched the top 2-layer/256-d + dropout result.
- Dropout: Non-zero dropout lifted validation and test metrics in the 2-layer family (peak Test@20=0.3268 at dropout=0.25); zero-dropout variants trailed slightly but remained competitive.
- Learning rate: The strongest GCN runs favored `lr=0.005`; the best zero-dropout configuration used `lr=0.001` with 3 layers.
- Regularization extremes: The 0.5-dropout sanity checks under Config C underperformed (Test@20 ≤0.2065), suggesting over-regularization.
- Validation/Test coupling: Higher validation Hits@20 generally tracked better test scores; monitoring val@20 remains a reasonable early-selection proxy.

## Suggested follow-ups

1) Rerun `B_l3_h192_do0.0_lr0.001_bs32k` with ≥3 seeds to firm up variance and confirm stability.  
2) Seed-average `A_l2_h256_do0.25_lr0.005_bs64k` to verify the observed uplift and measure variance.  
3) Try a narrow LR sweep around 0.001–0.002 for the 3-layer, zero-dropout configs to see if they can close the remaining gap.  
4) Add early-stop checkpoints keyed to val@20 to reduce wasted epochs on weaker configs in future sweeps.

## Relative to the long DDI run

Reference run: `logs/ddi_gcn_all_20251210_033923/ddi_gcn_all.log` (2-layer GCN, hidden=256, dropout=0.5, lr=0.005, batch=65,536, **2000 epochs**, external features fused). Best metrics there: Hits@20 Valid=0.7031 (ep 1685), Hits@20 Test=0.7328 (best single-epoch test peak ≈0.7995 at ep 1570 with val=0.6889).

- Gap: Sweep configs (100 epochs, no external features) top out at Test@20=0.3268, far below the feature-rich, long-horizon run (Test@20≈0.73). The delta is driven by both richer inputs and substantially longer training.
- What might scale with more epochs:  
  - `A_l2_h256_do0.25_lr0.005_bs64k` already leads the short run; its moderate dropout should stay stable over longer schedules, especially without external features.  
  - `B_l3_h192_do0.0_lr0.001_bs32k` showed the strongest zero-dropout result; deeper model plus small LR likely benefits from longer training if paired with decay/early stop to avoid late overfitting.  
  - `A_l2_h192_do0.25_lr0.005_bs64k` is a lighter variant that could converge faster; worth extending to ~500–800 epochs as a mid-budget check.
- Expectation: Extending these to 500–800 epochs (with a simple LR decay or cosine schedule) should narrow the gap, but matching the 0.73–0.80 Test@20 range will likely require external feature fusion similar to the long run.
