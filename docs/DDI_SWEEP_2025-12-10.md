# DDI Sweep (2025-12-10)

Sweep location: `logs/ddi_sweeps_20251210_200632` (GCN encoder, `run_ddi_sweeps.py` defaults; 100 epochs, eval every 5, single seed, device=CUDA if available). Dataset snapshot: 4,267 nodes, 2,135,822 edges.

Primary metric: Hits@20 on the OGBL-DDI test split.

## Latest runs from new DDI logs (Dec 10, night)

| Run | Setup (GCN + MLP) | Epochs | Val@20 | Test@20 | Notes |
| --- | --- | --- | --- | --- | --- |
| `logs/ddi_gcn_all_20251210_033923/ddi_gcn_all.log` | 2 layers, hidden 256, dropout 0.5, lr 0.005, batch 65,536, **external features fused** | 2,000 | 0.7031 | **0.7328** | Best single-epoch test peak ≈0.7995 (ep 1570) with val=0.6889; strongest run overall. |
| `logs/ddi_gcn_20251210_030519/ddi_gcn.log` | 2 layers, hidden 256, dropout 0.5, lr 0.005, batch 65,536 | 200 | 0.5609 | 0.3984 | No external features; best short-budget plain GCN. |
| `logs/ddi_gcn_20251210_030143/ddi_gcn.log` | 2 layers, hidden 256, dropout 0.5, lr 0.005, batch 65,536 | 100 | 0.4048 | 0.2232 | Steady gains to 75–100 epochs; benefits from longer schedule. |
| `logs/ddi_gcn_20251210_030052/ddi_gcn.log` | 2 layers, hidden 256, dropout 0.5, lr 0.005, batch 65,536 | 20 | 0.2475 | 0.0925 | Early-stop sanity check. |
| `logs/ddi_gcn_20251210_025937/ddi_gcn.log` | 2 layers, hidden 256, dropout 0.5, lr 0.005, batch 65,536 | 20 | 0.2111 | 0.1898 | Short warmup; better test than 030052 despite lower val. |
| `logs/ddi_long_sweeps_20251210_223703/sweep.log` | 2 layers, hidden 256, dropout 0.25, lr 0.005, batch 65,536 | 600+ | ~0.649 (best val) | ~0.424 (best observed test) | Single-config long sweep; no final summary block—values from mid-run peaks. |
| `logs/ddi_long_sweeps_20251210_231641/sweep.log` | 2 layers, hidden 256, dropout 0.5, lr 0.005, batch 65,536 | 330+ | 0.5894 (best val) | 0.5571 (best observed test) | Higher dropout variant; peaks mid-run without final summary. |

Takeaways:
- External feature fusion plus long training (GCN_all) is the current leader (Test@20≈0.73).
- Plain GCN with the same lr/dropout/batch reaches ~0.56/0.40 by 200 epochs; additional epochs and light lr decay likely help.
- Long sweeps show mid-run test peaks up to ~0.56 with dropout=0.5 and ~0.42 with dropout=0.25; add explicit early-stop summaries to capture best checkpoints.

## Top configs by Hits@20 (test)

| Config | Layers | Hidden | Dropout | LR | Batch | Val@20 | Test@20 | Log |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| A_l2_h256_do0.25_lr0.005_bs64k | 2 | 256 | 0.25 | 0.005 | 65,536 | 0.3860 | **0.3268** | `logs/ddi_sweeps_20251210_200632/A_l2_h256_do0.25_lr0.005_bs64k/config.log` |
| B_l3_h192_do0.0_lr0.001_bs32k | 3 | 192 | 0.0 | 0.001 | 32,768 | 0.2761 | **0.3066** | `logs/ddi_sweeps_20251210_200632/B_l3_h192_do0.0_lr0.001_bs32k/config.log` |
| A_l2_h192_do0.25_lr0.005_bs64k | 2 | 192 | 0.25 | 0.005 | 65,536 | 0.3936 | 0.2779 | `logs/ddi_sweeps_20251210_200632/A_l2_h192_do0.25_lr0.005_bs64k/config.log` |
| A_l2_h128_do0.1_lr0.005_bs64k | 2 | 128 | 0.1 | 0.005 | 65,536 | 0.3123 | 0.2744 | `logs/ddi_sweeps_20251210_200632/A_l2_h128_do0.1_lr0.005_bs64k/config.log` |
| A_l2_h192_do0.1_lr0.005_bs64k | 2 | 192 | 0.1 | 0.005 | 65,536 | 0.3607 | 0.2689 | `logs/ddi_sweeps_20251210_200632/A_l2_h192_do0.1_lr0.005_bs64k/config.log` |

Best zero-dropout option is `B_l3_h192_do0.0_lr0.001_bs32k` (Test@20=0.3066), second overall.

## Observations

- Depth/width: Larger hidden widths helped across families; 3-layer/192-d (dropout=0) nearly matched the top 2-layer/256-d + dropout result.
- Dropout: Non-zero dropout lifted validation and test metrics in the 2-layer family (peak Test@20=0.3268 at dropout=0.25); zero-dropout variants trailed slightly but remained competitive.
- Learning rate: The strongest GCN runs favored `lr=0.005`; the best zero-dropout configuration used `lr=0.001` with 3 layers.
- Regularization extremes: The 0.5-dropout sanity checks under Config C underperformed (Test@20 ≤0.2065), suggesting over-regularization.
- Validation/Test coupling: Higher validation Hits@20 generally tracked better test scores; monitoring val@20 remains a reasonable early-selection proxy.

## Suggested follow-ups

1) Rerun `B_l3_h192_do0.0_lr0.001_bs32k` with ≥3 seeds to firm up variance and confirm stability.  
2) Seed-average `A_l2_h256_do0.25_lr0.005_bs64k` to verify the observed uplift and measure variance.  
3) Try a narrow LR sweep around 0.001–0.002 for the 3-layer, zero-dropout configs to see if they can close the remaining gap.  
4) Add early-stop checkpoints keyed to val@20 to reduce wasted epochs on weaker configs in future sweeps.

## Relative to the long DDI run

Reference run: `logs/ddi_gcn_all_20251210_033923/ddi_gcn_all.log` (2-layer GCN, hidden=256, dropout=0.5, lr=0.005, batch=65,536, **2000 epochs**, external features fused). Best metrics there: Hits@20 Valid=0.7031 (ep 1685), Hits@20 Test=0.7328 (best single-epoch test peak ≈0.7995 at ep 1570 with val=0.6889).

- Gap: Sweep configs (100 epochs, no external features) top out at Test@20=0.3268, far below the feature-rich, long-horizon run (Test@20≈0.73). The delta is driven by both richer inputs and substantially longer training.
- What might scale with more epochs:  
  - `A_l2_h256_do0.25_lr0.005_bs64k` already leads the short run; its moderate dropout should stay stable over longer schedules, especially without external features.  
  - `B_l3_h192_do0.0_lr0.001_bs32k` showed the strongest zero-dropout result; deeper model plus small LR likely benefits from longer training if paired with decay/early stop to avoid late overfitting.  
  - `A_l2_h192_do0.25_lr0.005_bs64k` is a lighter variant that could converge faster; worth extending to ~500–800 epochs as a mid-budget check.
- Expectation: Extending these to 500–800 epochs (with a simple LR decay or cosine schedule) should narrow the gap, but matching the 0.73–0.80 Test@20 range will likely require external feature fusion similar to the long run.

## Underperforming configs (by Test@20)

Bottom performers in the sweep (Test@20):
- `B_l3_h192_do0.0_lr0.003_bs32k` — 0.0724 (likely too large LR for the 3-layer zero-dropout setup).
- `C_l2_h256_do0.5_lr0.005_bs64k` — 0.1136 (high dropout over-regularized).
- `B_l3_h128_do0.0_lr0.003_bs32k` — 0.1309 (capacity + LR combination underfit/unstable).
- `A_l2_h128_do0.25_lr0.003_bs64k` — 0.1463 (smaller width plus dropout underperformed).
- `A_l2_h128_do0.25_lr0.005_bs64k` — 0.1492 (same issue at higher LR).
- `A_l2_h192_do0.1_lr0.003_bs64k` — 0.1540 (mildly better but still lagging vs. other 192-d settings).
