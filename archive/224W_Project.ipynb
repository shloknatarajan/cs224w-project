{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio torch-geometric\n",
        "!pip install --quiet torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install --quiet torch-geometric\n",
        "!pip install --quiet ogb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-m8Ez2DqP8k",
        "outputId": "2a2b8656-c3fd-414d-f98f-5ad89838b914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m838.3/838.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m816.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m896.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, TransformerConv\n",
        "from torch_geometric.utils import negative_sampling\n",
        "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -------------------------------\n",
        "# Load Dataset (NO LEAKAGE)\n",
        "# -------------------------------\n",
        "dataset = PygLinkPropPredDataset('ogbl-ddi')\n",
        "data = dataset[0]\n",
        "\n",
        "split_edge = dataset.get_edge_split()\n",
        "\n",
        "train_pos = split_edge['train']['edge'].to(device)\n",
        "valid_pos = split_edge['valid']['edge'].to(device)\n",
        "test_pos  = split_edge['test']['edge'].to(device)\n",
        "\n",
        "num_nodes = data.num_nodes\n",
        "\n",
        "# Construct graph using *only* training edges (IMPORTANT: prevents leakage)\n",
        "data.edge_index = train_pos.t().contiguous().to(device)\n",
        "\n",
        "evaluator = Evaluator(name='ogbl-ddi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT77s2hlqckp",
        "outputId": "cdb228d8-e164-4c36-ddb6-6451b3b0971b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading http://snap.stanford.edu/ogb/data/linkproppred/ddi.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.04 GB: 100%|██████████| 46/46 [00:06<00:00,  7.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/ddi.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 50.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 3258.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n",
            "/usr/local/lib/python3.12/dist-packages/ogb/linkproppred/dataset_pyg.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
            "/usr/local/lib/python3.12/dist-packages/ogb/linkproppred/dataset_pyg.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'train.pt')))\n",
            "/usr/local/lib/python3.12/dist-packages/ogb/linkproppred/dataset_pyg.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  valid = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'valid.pt')))\n",
            "/usr/local/lib/python3.12/dist-packages/ogb/linkproppred/dataset_pyg.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'test.pt')))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def decode(self, z, edge):\n",
        "        src, dst = edge[:, 0], edge[:, 1]\n",
        "        return (z[src] * z[dst]).sum(dim=1)"
      ],
      "metadata": {
        "id": "GvTVU6NHwswv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(BaseModel):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "    def encode(self, edge_index):\n",
        "        # Random non-trainable node features → prevents memorization\n",
        "        x = torch.randn(num_nodes, self.hidden_dim, device=device)\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MLQa4n45rxU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphSAGE(BaseModel):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.conv1 = SAGEConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "\n",
        "    def encode(self, edge_index):\n",
        "        x = torch.randn(num_nodes, self.hidden_dim, device=device)\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zw4TCycFuCho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphTransformer(BaseModel):\n",
        "    def __init__(self, hidden_dim, heads=2):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.conv1 = TransformerConv(hidden_dim, hidden_dim // heads, heads=heads)\n",
        "        self.conv2 = TransformerConv(hidden_dim, hidden_dim // heads, heads=heads)\n",
        "\n",
        "    def encode(self, edge_index):\n",
        "        x = torch.randn(num_nodes, self.hidden_dim, device=device)\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AlvioErZuDjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(model, edge_index, pos_edges):\n",
        "    z = model.encode(edge_index)\n",
        "    pos_score = model.decode(z, pos_edges)\n",
        "\n",
        "    neg_edges = negative_sampling(\n",
        "        edge_index=edge_index,\n",
        "        num_nodes=num_nodes,\n",
        "        num_neg_samples=pos_edges.size(0)\n",
        "    ).to(device)\n",
        "\n",
        "    neg_score = model.decode(z, neg_edges)\n",
        "\n",
        "    # Margin ranking hinge loss\n",
        "    loss = -torch.log(torch.sigmoid(pos_score)).mean() - torch.log(1 - torch.sigmoid(neg_score)).mean()\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "IbnTsS87uFxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(data.edge_index)\n",
        "\n",
        "        # Positive scores\n",
        "        pos_scores = model.decode(z, test_pos).view(-1).cpu()\n",
        "\n",
        "        # Negative sampling + batch to avoid OOM\n",
        "        neg_test = negative_sampling(\n",
        "            edge_index=data.edge_index.cpu(),\n",
        "            num_nodes=num_nodes,\n",
        "            num_neg_samples=test_pos.size(0)\n",
        "        )\n",
        "        neg_test = neg_test.t().to(device)\n",
        "\n",
        "        neg_scores_list = []\n",
        "        batch = 200000  # safe chunk size\n",
        "        for i in range(0, neg_test.size(0), batch):\n",
        "            chunk = neg_test[i:i+batch]\n",
        "            neg_scores_list.append(model.decode(z, chunk).view(-1).cpu())\n",
        "\n",
        "        neg_scores = torch.cat(neg_scores_list)\n",
        "\n",
        "        result = evaluator.eval({\n",
        "            'y_pred_pos': pos_scores,\n",
        "            'y_pred_neg': neg_scores,\n",
        "        })\n",
        "\n",
        "        return result['hits@20']"
      ],
      "metadata": {
        "id": "F98y6Limry6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(name, model, epochs=1000, lr=0.001):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        z = model.encode(data.edge_index)\n",
        "        neg_samples = negative_sampling(\n",
        "            edge_index=data.edge_index,\n",
        "            num_nodes=num_nodes,\n",
        "            num_neg_samples=train_pos.size(0)\n",
        "        ).t().to(device)\n",
        "\n",
        "        pos_out = model.decode(z, train_pos).sigmoid()\n",
        "        neg_out = model.decode(z, neg_samples).sigmoid()\n",
        "\n",
        "        loss = -torch.log(pos_out + 1e-15).mean() - torch.log(1 - neg_out + 1e-15).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f\"{name} Epoch {epoch}/{epochs} - Loss = {loss.item():.4f}\")\n",
        "\n",
        "    hits20 = evaluate(model)\n",
        "    print(f\"{name}: Hits@20 = {hits20:.4f}\")\n",
        "    return hits20"
      ],
      "metadata": {
        "id": "I2y3H58Jr0yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcn_hits  = train_model(\"GCN\", GCN(128))\n",
        "sage_hits = train_model(\"GraphSAGE\", GraphSAGE(128))\n",
        "gt_hits   = train_model(\"GraphTransformer\", GraphTransformer(128))\n",
        "\n",
        "print(\"\\n==== FINAL RESULTS ====\")\n",
        "print(f\"GCN Hits@20       = {gcn_hits:.4f}\")\n",
        "print(f\"GraphSAGE Hits@20 = {sage_hits:.4f}\")\n",
        "print(f\"Transformer Hits@20 = {gt_hits:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8rsbITFr3dZ",
        "outputId": "3f0d2824-b362-4106-dac6-201a8207a979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN Epoch 5/1000 - Loss = 4.0406\n",
            "GCN Epoch 10/1000 - Loss = 1.8138\n",
            "GCN Epoch 15/1000 - Loss = 2.3580\n",
            "GCN Epoch 20/1000 - Loss = 1.7851\n",
            "GCN Epoch 25/1000 - Loss = 1.4486\n",
            "GCN Epoch 30/1000 - Loss = 1.6299\n",
            "GCN Epoch 35/1000 - Loss = 1.4335\n",
            "GCN Epoch 40/1000 - Loss = 1.4723\n",
            "GCN Epoch 45/1000 - Loss = 1.4954\n",
            "GCN Epoch 50/1000 - Loss = 1.8235\n",
            "GCN Epoch 55/1000 - Loss = 1.4470\n",
            "GCN Epoch 60/1000 - Loss = 1.4004\n",
            "GCN Epoch 65/1000 - Loss = 1.3768\n",
            "GCN Epoch 70/1000 - Loss = 1.4690\n",
            "GCN Epoch 75/1000 - Loss = 1.4660\n",
            "GCN Epoch 80/1000 - Loss = 1.6700\n",
            "GCN Epoch 85/1000 - Loss = 1.4468\n",
            "GCN Epoch 90/1000 - Loss = 1.3983\n",
            "GCN Epoch 95/1000 - Loss = 1.5373\n",
            "GCN Epoch 100/1000 - Loss = 1.5461\n",
            "GCN Epoch 105/1000 - Loss = 1.3005\n",
            "GCN Epoch 110/1000 - Loss = 1.3751\n",
            "GCN Epoch 115/1000 - Loss = 1.3616\n",
            "GCN Epoch 120/1000 - Loss = 1.4850\n",
            "GCN Epoch 125/1000 - Loss = 1.4569\n",
            "GCN Epoch 130/1000 - Loss = 1.3237\n",
            "GCN Epoch 135/1000 - Loss = 1.3722\n",
            "GCN Epoch 140/1000 - Loss = 1.7339\n",
            "GCN Epoch 145/1000 - Loss = 1.3500\n",
            "GCN Epoch 150/1000 - Loss = 1.4290\n",
            "GCN Epoch 155/1000 - Loss = 1.3405\n",
            "GCN Epoch 160/1000 - Loss = 1.3008\n",
            "GCN Epoch 165/1000 - Loss = 1.4771\n",
            "GCN Epoch 170/1000 - Loss = 1.4032\n",
            "GCN Epoch 175/1000 - Loss = 1.5841\n",
            "GCN Epoch 180/1000 - Loss = 1.4649\n",
            "GCN Epoch 185/1000 - Loss = 1.2957\n",
            "GCN Epoch 190/1000 - Loss = 1.3465\n",
            "GCN Epoch 195/1000 - Loss = 1.2969\n",
            "GCN Epoch 200/1000 - Loss = 1.2201\n",
            "GCN Epoch 205/1000 - Loss = 1.3886\n",
            "GCN Epoch 210/1000 - Loss = 1.3196\n",
            "GCN Epoch 215/1000 - Loss = 1.3924\n",
            "GCN Epoch 220/1000 - Loss = 1.3260\n",
            "GCN Epoch 225/1000 - Loss = 1.2560\n",
            "GCN Epoch 230/1000 - Loss = 1.2465\n",
            "GCN Epoch 235/1000 - Loss = 1.2711\n",
            "GCN Epoch 240/1000 - Loss = 1.4266\n",
            "GCN Epoch 245/1000 - Loss = 1.3264\n",
            "GCN Epoch 250/1000 - Loss = 1.2964\n",
            "GCN Epoch 255/1000 - Loss = 1.2158\n",
            "GCN Epoch 260/1000 - Loss = 1.2661\n",
            "GCN Epoch 265/1000 - Loss = 1.2543\n",
            "GCN Epoch 270/1000 - Loss = 1.2992\n",
            "GCN Epoch 275/1000 - Loss = 1.2602\n",
            "GCN Epoch 280/1000 - Loss = 1.2459\n",
            "GCN Epoch 285/1000 - Loss = 1.2306\n",
            "GCN Epoch 290/1000 - Loss = 1.2841\n",
            "GCN Epoch 295/1000 - Loss = 1.2428\n",
            "GCN Epoch 300/1000 - Loss = 1.2549\n",
            "GCN Epoch 305/1000 - Loss = 1.2680\n",
            "GCN Epoch 310/1000 - Loss = 1.2497\n",
            "GCN Epoch 315/1000 - Loss = 1.3497\n",
            "GCN Epoch 320/1000 - Loss = 1.2735\n",
            "GCN Epoch 325/1000 - Loss = 1.2658\n",
            "GCN Epoch 330/1000 - Loss = 1.2899\n",
            "GCN Epoch 335/1000 - Loss = 1.2921\n",
            "GCN Epoch 340/1000 - Loss = 1.2599\n",
            "GCN Epoch 345/1000 - Loss = 1.2285\n",
            "GCN Epoch 350/1000 - Loss = 1.2849\n",
            "GCN Epoch 355/1000 - Loss = 1.2008\n",
            "GCN Epoch 360/1000 - Loss = 1.2589\n",
            "GCN Epoch 365/1000 - Loss = 1.2527\n",
            "GCN Epoch 370/1000 - Loss = 1.2094\n",
            "GCN Epoch 375/1000 - Loss = 1.2442\n",
            "GCN Epoch 380/1000 - Loss = 1.2477\n",
            "GCN Epoch 385/1000 - Loss = 1.2482\n",
            "GCN Epoch 390/1000 - Loss = 1.3175\n",
            "GCN Epoch 395/1000 - Loss = 1.2331\n",
            "GCN Epoch 400/1000 - Loss = 1.2397\n",
            "GCN Epoch 405/1000 - Loss = 1.2278\n",
            "GCN Epoch 410/1000 - Loss = 1.2168\n",
            "GCN Epoch 415/1000 - Loss = 1.2280\n",
            "GCN Epoch 420/1000 - Loss = 1.2239\n",
            "GCN Epoch 425/1000 - Loss = 1.3137\n",
            "GCN Epoch 430/1000 - Loss = 1.2573\n",
            "GCN Epoch 435/1000 - Loss = 1.3038\n",
            "GCN Epoch 440/1000 - Loss = 1.2227\n",
            "GCN Epoch 445/1000 - Loss = 1.2260\n",
            "GCN Epoch 450/1000 - Loss = 1.2335\n",
            "GCN Epoch 455/1000 - Loss = 1.2288\n",
            "GCN Epoch 460/1000 - Loss = 1.2763\n",
            "GCN Epoch 465/1000 - Loss = 1.2188\n",
            "GCN Epoch 470/1000 - Loss = 1.2338\n",
            "GCN Epoch 475/1000 - Loss = 1.2474\n",
            "GCN Epoch 480/1000 - Loss = 1.3425\n",
            "GCN Epoch 485/1000 - Loss = 1.2380\n",
            "GCN Epoch 490/1000 - Loss = 1.2875\n",
            "GCN Epoch 495/1000 - Loss = 1.2613\n",
            "GCN Epoch 500/1000 - Loss = 1.2155\n",
            "GCN Epoch 505/1000 - Loss = 1.2598\n",
            "GCN Epoch 510/1000 - Loss = 1.2269\n",
            "GCN Epoch 515/1000 - Loss = 1.2238\n",
            "GCN Epoch 520/1000 - Loss = 1.2317\n",
            "GCN Epoch 525/1000 - Loss = 1.2625\n",
            "GCN Epoch 530/1000 - Loss = 1.2675\n",
            "GCN Epoch 535/1000 - Loss = 1.2433\n",
            "GCN Epoch 540/1000 - Loss = 1.2245\n",
            "GCN Epoch 545/1000 - Loss = 1.2639\n",
            "GCN Epoch 550/1000 - Loss = 1.2316\n",
            "GCN Epoch 555/1000 - Loss = 1.2607\n",
            "GCN Epoch 560/1000 - Loss = 1.3005\n",
            "GCN Epoch 565/1000 - Loss = 1.2359\n",
            "GCN Epoch 570/1000 - Loss = 1.2182\n",
            "GCN Epoch 575/1000 - Loss = 1.2498\n",
            "GCN Epoch 580/1000 - Loss = 1.2663\n",
            "GCN Epoch 585/1000 - Loss = 1.2793\n",
            "GCN Epoch 590/1000 - Loss = 1.2634\n",
            "GCN Epoch 595/1000 - Loss = 1.2137\n",
            "GCN Epoch 600/1000 - Loss = 1.2436\n",
            "GCN Epoch 605/1000 - Loss = 1.2444\n",
            "GCN Epoch 610/1000 - Loss = 1.2332\n",
            "GCN Epoch 615/1000 - Loss = 1.2354\n",
            "GCN Epoch 620/1000 - Loss = 1.2427\n",
            "GCN Epoch 625/1000 - Loss = 1.2476\n",
            "GCN Epoch 630/1000 - Loss = 1.2240\n",
            "GCN Epoch 635/1000 - Loss = 1.2194\n",
            "GCN Epoch 640/1000 - Loss = 1.2383\n",
            "GCN Epoch 645/1000 - Loss = 1.2283\n",
            "GCN Epoch 650/1000 - Loss = 1.2303\n",
            "GCN Epoch 655/1000 - Loss = 1.3042\n",
            "GCN Epoch 660/1000 - Loss = 1.2614\n",
            "GCN Epoch 665/1000 - Loss = 1.2426\n",
            "GCN Epoch 670/1000 - Loss = 1.2753\n",
            "GCN Epoch 675/1000 - Loss = 1.2232\n",
            "GCN Epoch 680/1000 - Loss = 1.2152\n",
            "GCN Epoch 685/1000 - Loss = 1.2298\n",
            "GCN Epoch 690/1000 - Loss = 1.2476\n",
            "GCN Epoch 695/1000 - Loss = 1.2294\n",
            "GCN Epoch 700/1000 - Loss = 1.2378\n",
            "GCN Epoch 705/1000 - Loss = 1.2395\n",
            "GCN Epoch 710/1000 - Loss = 1.2367\n",
            "GCN Epoch 715/1000 - Loss = 1.2392\n",
            "GCN Epoch 720/1000 - Loss = 1.2179\n",
            "GCN Epoch 725/1000 - Loss = 1.2146\n",
            "GCN Epoch 730/1000 - Loss = 1.2129\n",
            "GCN Epoch 735/1000 - Loss = 1.2363\n",
            "GCN Epoch 740/1000 - Loss = 1.2314\n",
            "GCN Epoch 745/1000 - Loss = 1.2152\n",
            "GCN Epoch 750/1000 - Loss = 1.2516\n",
            "GCN Epoch 755/1000 - Loss = 1.2427\n",
            "GCN Epoch 760/1000 - Loss = 1.2298\n",
            "GCN Epoch 765/1000 - Loss = 1.2236\n",
            "GCN Epoch 770/1000 - Loss = 1.2431\n",
            "GCN Epoch 775/1000 - Loss = 1.2338\n",
            "GCN Epoch 780/1000 - Loss = 1.2228\n",
            "GCN Epoch 785/1000 - Loss = 1.2204\n",
            "GCN Epoch 790/1000 - Loss = 1.2287\n",
            "GCN Epoch 795/1000 - Loss = 1.2428\n",
            "GCN Epoch 800/1000 - Loss = 1.2267\n",
            "GCN Epoch 805/1000 - Loss = 1.2284\n",
            "GCN Epoch 810/1000 - Loss = 1.2633\n",
            "GCN Epoch 815/1000 - Loss = 1.2174\n",
            "GCN Epoch 820/1000 - Loss = 1.2371\n",
            "GCN Epoch 825/1000 - Loss = 1.2540\n",
            "GCN Epoch 830/1000 - Loss = 1.2566\n",
            "GCN Epoch 835/1000 - Loss = 1.2391\n",
            "GCN Epoch 840/1000 - Loss = 1.2365\n",
            "GCN Epoch 845/1000 - Loss = 1.2349\n",
            "GCN Epoch 850/1000 - Loss = 1.2188\n",
            "GCN Epoch 855/1000 - Loss = 1.2177\n",
            "GCN Epoch 860/1000 - Loss = 1.2341\n",
            "GCN Epoch 865/1000 - Loss = 1.2161\n",
            "GCN Epoch 870/1000 - Loss = 1.2182\n",
            "GCN Epoch 875/1000 - Loss = 1.2233\n",
            "GCN Epoch 880/1000 - Loss = 1.2278\n",
            "GCN Epoch 885/1000 - Loss = 1.2382\n",
            "GCN Epoch 890/1000 - Loss = 1.2474\n",
            "GCN Epoch 895/1000 - Loss = 1.2465\n",
            "GCN Epoch 900/1000 - Loss = 1.2194\n",
            "GCN Epoch 905/1000 - Loss = 1.2489\n",
            "GCN Epoch 910/1000 - Loss = 1.2266\n",
            "GCN Epoch 915/1000 - Loss = 1.2353\n",
            "GCN Epoch 920/1000 - Loss = 1.2052\n",
            "GCN Epoch 925/1000 - Loss = 1.2523\n",
            "GCN Epoch 930/1000 - Loss = 1.2442\n",
            "GCN Epoch 935/1000 - Loss = 1.2182\n",
            "GCN Epoch 940/1000 - Loss = 1.2375\n",
            "GCN Epoch 945/1000 - Loss = 1.2391\n",
            "GCN Epoch 950/1000 - Loss = 1.2265\n",
            "GCN Epoch 955/1000 - Loss = 1.2157\n",
            "GCN Epoch 960/1000 - Loss = 1.2343\n",
            "GCN Epoch 965/1000 - Loss = 1.2116\n",
            "GCN Epoch 970/1000 - Loss = 1.2281\n",
            "GCN Epoch 975/1000 - Loss = 1.2215\n",
            "GCN Epoch 980/1000 - Loss = 1.2264\n",
            "GCN Epoch 985/1000 - Loss = 1.2416\n",
            "GCN Epoch 990/1000 - Loss = 1.2562\n",
            "GCN Epoch 995/1000 - Loss = 1.2348\n",
            "GCN Epoch 1000/1000 - Loss = 1.2554\n",
            "GCN: Hits@20 = 0.0007\n",
            "GraphSAGE Epoch 5/1000 - Loss = 1.5973\n",
            "GraphSAGE Epoch 10/1000 - Loss = 1.6563\n",
            "GraphSAGE Epoch 15/1000 - Loss = 1.5465\n",
            "GraphSAGE Epoch 20/1000 - Loss = 1.4858\n",
            "GraphSAGE Epoch 25/1000 - Loss = 1.4611\n",
            "GraphSAGE Epoch 30/1000 - Loss = 1.4425\n",
            "GraphSAGE Epoch 35/1000 - Loss = 1.4216\n",
            "GraphSAGE Epoch 40/1000 - Loss = 1.4077\n",
            "GraphSAGE Epoch 45/1000 - Loss = 1.4052\n",
            "GraphSAGE Epoch 50/1000 - Loss = 1.4029\n",
            "GraphSAGE Epoch 55/1000 - Loss = 1.4039\n",
            "GraphSAGE Epoch 60/1000 - Loss = 1.3944\n",
            "GraphSAGE Epoch 65/1000 - Loss = 1.4002\n",
            "GraphSAGE Epoch 70/1000 - Loss = 1.3968\n",
            "GraphSAGE Epoch 75/1000 - Loss = 1.3912\n",
            "GraphSAGE Epoch 80/1000 - Loss = 1.3826\n",
            "GraphSAGE Epoch 85/1000 - Loss = 1.3830\n",
            "GraphSAGE Epoch 90/1000 - Loss = 1.3871\n",
            "GraphSAGE Epoch 95/1000 - Loss = 1.3791\n",
            "GraphSAGE Epoch 100/1000 - Loss = 1.3771\n",
            "GraphSAGE Epoch 105/1000 - Loss = 1.3739\n",
            "GraphSAGE Epoch 110/1000 - Loss = 1.3738\n",
            "GraphSAGE Epoch 115/1000 - Loss = 1.3734\n",
            "GraphSAGE Epoch 120/1000 - Loss = 1.3735\n",
            "GraphSAGE Epoch 125/1000 - Loss = 1.3733\n",
            "GraphSAGE Epoch 130/1000 - Loss = 1.3623\n",
            "GraphSAGE Epoch 135/1000 - Loss = 1.3638\n",
            "GraphSAGE Epoch 140/1000 - Loss = 1.3598\n",
            "GraphSAGE Epoch 145/1000 - Loss = 1.3577\n",
            "GraphSAGE Epoch 150/1000 - Loss = 1.3565\n",
            "GraphSAGE Epoch 155/1000 - Loss = 1.3605\n",
            "GraphSAGE Epoch 160/1000 - Loss = 1.3561\n",
            "GraphSAGE Epoch 165/1000 - Loss = 1.3494\n",
            "GraphSAGE Epoch 170/1000 - Loss = 1.3494\n",
            "GraphSAGE Epoch 175/1000 - Loss = 1.3517\n",
            "GraphSAGE Epoch 180/1000 - Loss = 1.3422\n",
            "GraphSAGE Epoch 185/1000 - Loss = 1.3530\n",
            "GraphSAGE Epoch 190/1000 - Loss = 1.3450\n",
            "GraphSAGE Epoch 195/1000 - Loss = 1.3374\n",
            "GraphSAGE Epoch 200/1000 - Loss = 1.3397\n",
            "GraphSAGE Epoch 205/1000 - Loss = 1.3430\n",
            "GraphSAGE Epoch 210/1000 - Loss = 1.3330\n",
            "GraphSAGE Epoch 215/1000 - Loss = 1.3285\n",
            "GraphSAGE Epoch 220/1000 - Loss = 1.3362\n",
            "GraphSAGE Epoch 225/1000 - Loss = 1.3283\n",
            "GraphSAGE Epoch 230/1000 - Loss = 1.3262\n",
            "GraphSAGE Epoch 235/1000 - Loss = 1.3341\n",
            "GraphSAGE Epoch 240/1000 - Loss = 1.3204\n",
            "GraphSAGE Epoch 245/1000 - Loss = 1.3238\n",
            "GraphSAGE Epoch 250/1000 - Loss = 1.3219\n",
            "GraphSAGE Epoch 255/1000 - Loss = 1.3225\n",
            "GraphSAGE Epoch 260/1000 - Loss = 1.3251\n",
            "GraphSAGE Epoch 265/1000 - Loss = 1.3088\n",
            "GraphSAGE Epoch 270/1000 - Loss = 1.3123\n",
            "GraphSAGE Epoch 275/1000 - Loss = 1.3059\n",
            "GraphSAGE Epoch 280/1000 - Loss = 1.3103\n",
            "GraphSAGE Epoch 285/1000 - Loss = 1.3109\n",
            "GraphSAGE Epoch 290/1000 - Loss = 1.3036\n",
            "GraphSAGE Epoch 295/1000 - Loss = 1.3018\n",
            "GraphSAGE Epoch 300/1000 - Loss = 1.3056\n",
            "GraphSAGE Epoch 305/1000 - Loss = 1.3027\n",
            "GraphSAGE Epoch 310/1000 - Loss = 1.3012\n",
            "GraphSAGE Epoch 315/1000 - Loss = 1.3103\n",
            "GraphSAGE Epoch 320/1000 - Loss = 1.2957\n",
            "GraphSAGE Epoch 325/1000 - Loss = 1.3005\n",
            "GraphSAGE Epoch 330/1000 - Loss = 1.2993\n",
            "GraphSAGE Epoch 335/1000 - Loss = 1.3053\n",
            "GraphSAGE Epoch 340/1000 - Loss = 1.2996\n",
            "GraphSAGE Epoch 345/1000 - Loss = 1.2951\n",
            "GraphSAGE Epoch 350/1000 - Loss = 1.2952\n",
            "GraphSAGE Epoch 355/1000 - Loss = 1.3034\n",
            "GraphSAGE Epoch 360/1000 - Loss = 1.2995\n",
            "GraphSAGE Epoch 365/1000 - Loss = 1.3099\n",
            "GraphSAGE Epoch 370/1000 - Loss = 1.2934\n",
            "GraphSAGE Epoch 375/1000 - Loss = 1.2957\n",
            "GraphSAGE Epoch 380/1000 - Loss = 1.2883\n",
            "GraphSAGE Epoch 385/1000 - Loss = 1.2971\n",
            "GraphSAGE Epoch 390/1000 - Loss = 1.2964\n",
            "GraphSAGE Epoch 395/1000 - Loss = 1.2947\n",
            "GraphSAGE Epoch 400/1000 - Loss = 1.2951\n",
            "GraphSAGE Epoch 405/1000 - Loss = 1.2905\n",
            "GraphSAGE Epoch 410/1000 - Loss = 1.2954\n",
            "GraphSAGE Epoch 415/1000 - Loss = 1.2955\n",
            "GraphSAGE Epoch 420/1000 - Loss = 1.2895\n",
            "GraphSAGE Epoch 425/1000 - Loss = 1.3019\n",
            "GraphSAGE Epoch 430/1000 - Loss = 1.2911\n",
            "GraphSAGE Epoch 435/1000 - Loss = 1.2956\n",
            "GraphSAGE Epoch 440/1000 - Loss = 1.2989\n",
            "GraphSAGE Epoch 445/1000 - Loss = 1.2920\n",
            "GraphSAGE Epoch 450/1000 - Loss = 1.2876\n",
            "GraphSAGE Epoch 455/1000 - Loss = 1.2967\n",
            "GraphSAGE Epoch 460/1000 - Loss = 1.2924\n",
            "GraphSAGE Epoch 465/1000 - Loss = 1.2853\n",
            "GraphSAGE Epoch 470/1000 - Loss = 1.2918\n",
            "GraphSAGE Epoch 475/1000 - Loss = 1.2849\n",
            "GraphSAGE Epoch 480/1000 - Loss = 1.2934\n",
            "GraphSAGE Epoch 485/1000 - Loss = 1.2872\n",
            "GraphSAGE Epoch 490/1000 - Loss = 1.2921\n",
            "GraphSAGE Epoch 495/1000 - Loss = 1.2927\n",
            "GraphSAGE Epoch 500/1000 - Loss = 1.2941\n",
            "GraphSAGE Epoch 505/1000 - Loss = 1.2847\n",
            "GraphSAGE Epoch 510/1000 - Loss = 1.2882\n",
            "GraphSAGE Epoch 515/1000 - Loss = 1.3026\n",
            "GraphSAGE Epoch 520/1000 - Loss = 1.2896\n",
            "GraphSAGE Epoch 525/1000 - Loss = 1.2851\n",
            "GraphSAGE Epoch 530/1000 - Loss = 1.2920\n",
            "GraphSAGE Epoch 535/1000 - Loss = 1.2886\n",
            "GraphSAGE Epoch 540/1000 - Loss = 1.2857\n",
            "GraphSAGE Epoch 545/1000 - Loss = 1.2967\n",
            "GraphSAGE Epoch 550/1000 - Loss = 1.2859\n",
            "GraphSAGE Epoch 555/1000 - Loss = 1.2860\n",
            "GraphSAGE Epoch 560/1000 - Loss = 1.2787\n",
            "GraphSAGE Epoch 565/1000 - Loss = 1.2933\n",
            "GraphSAGE Epoch 570/1000 - Loss = 1.2852\n",
            "GraphSAGE Epoch 575/1000 - Loss = 1.2822\n",
            "GraphSAGE Epoch 580/1000 - Loss = 1.2835\n",
            "GraphSAGE Epoch 585/1000 - Loss = 1.2897\n",
            "GraphSAGE Epoch 590/1000 - Loss = 1.2889\n",
            "GraphSAGE Epoch 595/1000 - Loss = 1.2942\n",
            "GraphSAGE Epoch 600/1000 - Loss = 1.2912\n",
            "GraphSAGE Epoch 605/1000 - Loss = 1.2877\n",
            "GraphSAGE Epoch 610/1000 - Loss = 1.2938\n",
            "GraphSAGE Epoch 615/1000 - Loss = 1.2805\n",
            "GraphSAGE Epoch 620/1000 - Loss = 1.2900\n",
            "GraphSAGE Epoch 625/1000 - Loss = 1.2820\n",
            "GraphSAGE Epoch 630/1000 - Loss = 1.2781\n",
            "GraphSAGE Epoch 635/1000 - Loss = 1.2865\n",
            "GraphSAGE Epoch 640/1000 - Loss = 1.2861\n",
            "GraphSAGE Epoch 645/1000 - Loss = 1.2874\n",
            "GraphSAGE Epoch 650/1000 - Loss = 1.2855\n",
            "GraphSAGE Epoch 655/1000 - Loss = 1.2818\n",
            "GraphSAGE Epoch 660/1000 - Loss = 1.2877\n",
            "GraphSAGE Epoch 665/1000 - Loss = 1.2749\n",
            "GraphSAGE Epoch 670/1000 - Loss = 1.2877\n",
            "GraphSAGE Epoch 675/1000 - Loss = 1.2811\n",
            "GraphSAGE Epoch 680/1000 - Loss = 1.2852\n",
            "GraphSAGE Epoch 685/1000 - Loss = 1.2827\n",
            "GraphSAGE Epoch 690/1000 - Loss = 1.2819\n",
            "GraphSAGE Epoch 695/1000 - Loss = 1.2773\n",
            "GraphSAGE Epoch 700/1000 - Loss = 1.2902\n",
            "GraphSAGE Epoch 705/1000 - Loss = 1.2800\n",
            "GraphSAGE Epoch 710/1000 - Loss = 1.2917\n",
            "GraphSAGE Epoch 715/1000 - Loss = 1.2809\n",
            "GraphSAGE Epoch 720/1000 - Loss = 1.2815\n",
            "GraphSAGE Epoch 725/1000 - Loss = 1.2807\n",
            "GraphSAGE Epoch 730/1000 - Loss = 1.2809\n",
            "GraphSAGE Epoch 735/1000 - Loss = 1.2777\n",
            "GraphSAGE Epoch 740/1000 - Loss = 1.2819\n",
            "GraphSAGE Epoch 745/1000 - Loss = 1.2859\n",
            "GraphSAGE Epoch 750/1000 - Loss = 1.2837\n",
            "GraphSAGE Epoch 755/1000 - Loss = 1.2829\n",
            "GraphSAGE Epoch 760/1000 - Loss = 1.2892\n",
            "GraphSAGE Epoch 765/1000 - Loss = 1.2892\n",
            "GraphSAGE Epoch 770/1000 - Loss = 1.2847\n",
            "GraphSAGE Epoch 775/1000 - Loss = 1.2751\n",
            "GraphSAGE Epoch 780/1000 - Loss = 1.2798\n",
            "GraphSAGE Epoch 785/1000 - Loss = 1.2832\n",
            "GraphSAGE Epoch 790/1000 - Loss = 1.2833\n",
            "GraphSAGE Epoch 795/1000 - Loss = 1.2797\n",
            "GraphSAGE Epoch 800/1000 - Loss = 1.2819\n",
            "GraphSAGE Epoch 805/1000 - Loss = 1.2856\n",
            "GraphSAGE Epoch 810/1000 - Loss = 1.2837\n",
            "GraphSAGE Epoch 815/1000 - Loss = 1.2772\n",
            "GraphSAGE Epoch 820/1000 - Loss = 1.2845\n",
            "GraphSAGE Epoch 825/1000 - Loss = 1.2877\n",
            "GraphSAGE Epoch 830/1000 - Loss = 1.2847\n",
            "GraphSAGE Epoch 835/1000 - Loss = 1.2897\n",
            "GraphSAGE Epoch 840/1000 - Loss = 1.2890\n",
            "GraphSAGE Epoch 845/1000 - Loss = 1.2824\n",
            "GraphSAGE Epoch 850/1000 - Loss = 1.2830\n",
            "GraphSAGE Epoch 855/1000 - Loss = 1.2889\n",
            "GraphSAGE Epoch 860/1000 - Loss = 1.2822\n",
            "GraphSAGE Epoch 865/1000 - Loss = 1.2821\n",
            "GraphSAGE Epoch 870/1000 - Loss = 1.2767\n",
            "GraphSAGE Epoch 875/1000 - Loss = 1.2862\n",
            "GraphSAGE Epoch 880/1000 - Loss = 1.2765\n",
            "GraphSAGE Epoch 885/1000 - Loss = 1.2756\n",
            "GraphSAGE Epoch 890/1000 - Loss = 1.2863\n",
            "GraphSAGE Epoch 895/1000 - Loss = 1.2787\n",
            "GraphSAGE Epoch 900/1000 - Loss = 1.2845\n",
            "GraphSAGE Epoch 905/1000 - Loss = 1.2851\n",
            "GraphSAGE Epoch 910/1000 - Loss = 1.2855\n",
            "GraphSAGE Epoch 915/1000 - Loss = 1.2816\n",
            "GraphSAGE Epoch 920/1000 - Loss = 1.2797\n",
            "GraphSAGE Epoch 925/1000 - Loss = 1.2836\n",
            "GraphSAGE Epoch 930/1000 - Loss = 1.2831\n",
            "GraphSAGE Epoch 935/1000 - Loss = 1.2762\n",
            "GraphSAGE Epoch 940/1000 - Loss = 1.2835\n",
            "GraphSAGE Epoch 945/1000 - Loss = 1.2872\n",
            "GraphSAGE Epoch 950/1000 - Loss = 1.2827\n",
            "GraphSAGE Epoch 955/1000 - Loss = 1.2764\n",
            "GraphSAGE Epoch 960/1000 - Loss = 1.2761\n",
            "GraphSAGE Epoch 965/1000 - Loss = 1.2755\n",
            "GraphSAGE Epoch 970/1000 - Loss = 1.2897\n",
            "GraphSAGE Epoch 975/1000 - Loss = 1.2826\n",
            "GraphSAGE Epoch 980/1000 - Loss = 1.2782\n",
            "GraphSAGE Epoch 985/1000 - Loss = 1.2785\n",
            "GraphSAGE Epoch 990/1000 - Loss = 1.2815\n",
            "GraphSAGE Epoch 995/1000 - Loss = 1.2829\n",
            "GraphSAGE Epoch 1000/1000 - Loss = 1.2700\n",
            "GraphSAGE: Hits@20 = 0.0000\n",
            "GraphTransformer Epoch 5/1000 - Loss = 1.6045\n",
            "GraphTransformer Epoch 10/1000 - Loss = 1.6525\n",
            "GraphTransformer Epoch 15/1000 - Loss = 1.5492\n",
            "GraphTransformer Epoch 20/1000 - Loss = 1.4733\n",
            "GraphTransformer Epoch 25/1000 - Loss = 1.4533\n",
            "GraphTransformer Epoch 30/1000 - Loss = 1.4382\n",
            "GraphTransformer Epoch 35/1000 - Loss = 1.4201\n",
            "GraphTransformer Epoch 40/1000 - Loss = 1.4154\n",
            "GraphTransformer Epoch 45/1000 - Loss = 1.4034\n",
            "GraphTransformer Epoch 50/1000 - Loss = 1.4019\n",
            "GraphTransformer Epoch 55/1000 - Loss = 1.4015\n",
            "GraphTransformer Epoch 60/1000 - Loss = 1.3915\n",
            "GraphTransformer Epoch 65/1000 - Loss = 1.3883\n",
            "GraphTransformer Epoch 70/1000 - Loss = 1.3880\n",
            "GraphTransformer Epoch 75/1000 - Loss = 1.3878\n",
            "GraphTransformer Epoch 80/1000 - Loss = 1.3849\n",
            "GraphTransformer Epoch 85/1000 - Loss = 1.3774\n",
            "GraphTransformer Epoch 90/1000 - Loss = 1.3755\n",
            "GraphTransformer Epoch 95/1000 - Loss = 1.3834\n",
            "GraphTransformer Epoch 100/1000 - Loss = 1.3746\n",
            "GraphTransformer Epoch 105/1000 - Loss = 1.3690\n",
            "GraphTransformer Epoch 110/1000 - Loss = 1.3695\n",
            "GraphTransformer Epoch 115/1000 - Loss = 1.3715\n",
            "GraphTransformer Epoch 120/1000 - Loss = 1.3642\n",
            "GraphTransformer Epoch 125/1000 - Loss = 1.3580\n",
            "GraphTransformer Epoch 130/1000 - Loss = 1.3527\n",
            "GraphTransformer Epoch 135/1000 - Loss = 1.3565\n",
            "GraphTransformer Epoch 140/1000 - Loss = 1.3572\n",
            "GraphTransformer Epoch 145/1000 - Loss = 1.3498\n",
            "GraphTransformer Epoch 150/1000 - Loss = 1.3454\n",
            "GraphTransformer Epoch 155/1000 - Loss = 1.3441\n",
            "GraphTransformer Epoch 160/1000 - Loss = 1.3443\n",
            "GraphTransformer Epoch 165/1000 - Loss = 1.3407\n",
            "GraphTransformer Epoch 170/1000 - Loss = 1.3348\n",
            "GraphTransformer Epoch 175/1000 - Loss = 1.3429\n",
            "GraphTransformer Epoch 180/1000 - Loss = 1.3387\n",
            "GraphTransformer Epoch 185/1000 - Loss = 1.3334\n",
            "GraphTransformer Epoch 190/1000 - Loss = 1.3366\n",
            "GraphTransformer Epoch 195/1000 - Loss = 1.3317\n",
            "GraphTransformer Epoch 200/1000 - Loss = 1.3208\n",
            "GraphTransformer Epoch 205/1000 - Loss = 1.3206\n",
            "GraphTransformer Epoch 210/1000 - Loss = 1.3210\n",
            "GraphTransformer Epoch 215/1000 - Loss = 1.3052\n",
            "GraphTransformer Epoch 220/1000 - Loss = 1.3088\n",
            "GraphTransformer Epoch 225/1000 - Loss = 1.3027\n",
            "GraphTransformer Epoch 230/1000 - Loss = 1.3087\n",
            "GraphTransformer Epoch 235/1000 - Loss = 1.2973\n",
            "GraphTransformer Epoch 240/1000 - Loss = 1.2955\n",
            "GraphTransformer Epoch 245/1000 - Loss = 1.2943\n",
            "GraphTransformer Epoch 250/1000 - Loss = 1.2932\n",
            "GraphTransformer Epoch 255/1000 - Loss = 1.2844\n",
            "GraphTransformer Epoch 260/1000 - Loss = 1.2872\n",
            "GraphTransformer Epoch 265/1000 - Loss = 1.2875\n",
            "GraphTransformer Epoch 270/1000 - Loss = 1.2847\n",
            "GraphTransformer Epoch 275/1000 - Loss = 1.2824\n",
            "GraphTransformer Epoch 280/1000 - Loss = 1.2854\n",
            "GraphTransformer Epoch 285/1000 - Loss = 1.2764\n",
            "GraphTransformer Epoch 290/1000 - Loss = 1.2808\n",
            "GraphTransformer Epoch 295/1000 - Loss = 1.2820\n",
            "GraphTransformer Epoch 300/1000 - Loss = 1.2820\n",
            "GraphTransformer Epoch 305/1000 - Loss = 1.2797\n",
            "GraphTransformer Epoch 310/1000 - Loss = 1.2761\n",
            "GraphTransformer Epoch 315/1000 - Loss = 1.2786\n",
            "GraphTransformer Epoch 320/1000 - Loss = 1.2777\n",
            "GraphTransformer Epoch 325/1000 - Loss = 1.2788\n",
            "GraphTransformer Epoch 330/1000 - Loss = 1.2687\n",
            "GraphTransformer Epoch 335/1000 - Loss = 1.2728\n",
            "GraphTransformer Epoch 340/1000 - Loss = 1.2693\n",
            "GraphTransformer Epoch 345/1000 - Loss = 1.2747\n",
            "GraphTransformer Epoch 350/1000 - Loss = 1.2721\n",
            "GraphTransformer Epoch 355/1000 - Loss = 1.2706\n",
            "GraphTransformer Epoch 360/1000 - Loss = 1.2674\n",
            "GraphTransformer Epoch 365/1000 - Loss = 1.2677\n",
            "GraphTransformer Epoch 370/1000 - Loss = 1.2704\n",
            "GraphTransformer Epoch 375/1000 - Loss = 1.2686\n",
            "GraphTransformer Epoch 380/1000 - Loss = 1.2670\n",
            "GraphTransformer Epoch 385/1000 - Loss = 1.2693\n",
            "GraphTransformer Epoch 390/1000 - Loss = 1.2696\n",
            "GraphTransformer Epoch 395/1000 - Loss = 1.2667\n",
            "GraphTransformer Epoch 400/1000 - Loss = 1.2629\n",
            "GraphTransformer Epoch 405/1000 - Loss = 1.2683\n",
            "GraphTransformer Epoch 410/1000 - Loss = 1.2664\n",
            "GraphTransformer Epoch 415/1000 - Loss = 1.2662\n",
            "GraphTransformer Epoch 420/1000 - Loss = 1.2657\n",
            "GraphTransformer Epoch 425/1000 - Loss = 1.2628\n",
            "GraphTransformer Epoch 430/1000 - Loss = 1.2687\n",
            "GraphTransformer Epoch 435/1000 - Loss = 1.2658\n",
            "GraphTransformer Epoch 440/1000 - Loss = 1.2616\n",
            "GraphTransformer Epoch 445/1000 - Loss = 1.2752\n",
            "GraphTransformer Epoch 450/1000 - Loss = 1.2620\n",
            "GraphTransformer Epoch 455/1000 - Loss = 1.2632\n",
            "GraphTransformer Epoch 460/1000 - Loss = 1.2598\n",
            "GraphTransformer Epoch 465/1000 - Loss = 1.2694\n",
            "GraphTransformer Epoch 470/1000 - Loss = 1.2655\n",
            "GraphTransformer Epoch 475/1000 - Loss = 1.2632\n",
            "GraphTransformer Epoch 480/1000 - Loss = 1.2645\n",
            "GraphTransformer Epoch 485/1000 - Loss = 1.2597\n",
            "GraphTransformer Epoch 490/1000 - Loss = 1.2647\n",
            "GraphTransformer Epoch 495/1000 - Loss = 1.2642\n",
            "GraphTransformer Epoch 500/1000 - Loss = 1.2651\n",
            "GraphTransformer Epoch 505/1000 - Loss = 1.2677\n",
            "GraphTransformer Epoch 510/1000 - Loss = 1.2674\n",
            "GraphTransformer Epoch 515/1000 - Loss = 1.2648\n",
            "GraphTransformer Epoch 520/1000 - Loss = 1.2608\n",
            "GraphTransformer Epoch 525/1000 - Loss = 1.2639\n",
            "GraphTransformer Epoch 530/1000 - Loss = 1.2604\n",
            "GraphTransformer Epoch 535/1000 - Loss = 1.2613\n",
            "GraphTransformer Epoch 540/1000 - Loss = 1.2640\n",
            "GraphTransformer Epoch 545/1000 - Loss = 1.2614\n",
            "GraphTransformer Epoch 550/1000 - Loss = 1.2594\n",
            "GraphTransformer Epoch 555/1000 - Loss = 1.2613\n",
            "GraphTransformer Epoch 560/1000 - Loss = 1.2576\n",
            "GraphTransformer Epoch 565/1000 - Loss = 1.2575\n",
            "GraphTransformer Epoch 570/1000 - Loss = 1.2577\n",
            "GraphTransformer Epoch 575/1000 - Loss = 1.2628\n",
            "GraphTransformer Epoch 580/1000 - Loss = 1.2612\n",
            "GraphTransformer Epoch 585/1000 - Loss = 1.2535\n",
            "GraphTransformer Epoch 590/1000 - Loss = 1.2614\n",
            "GraphTransformer Epoch 595/1000 - Loss = 1.2581\n",
            "GraphTransformer Epoch 600/1000 - Loss = 1.2566\n",
            "GraphTransformer Epoch 605/1000 - Loss = 1.2601\n",
            "GraphTransformer Epoch 610/1000 - Loss = 1.2533\n",
            "GraphTransformer Epoch 615/1000 - Loss = 1.2547\n",
            "GraphTransformer Epoch 620/1000 - Loss = 1.2600\n",
            "GraphTransformer Epoch 625/1000 - Loss = 1.2576\n",
            "GraphTransformer Epoch 630/1000 - Loss = 1.2555\n",
            "GraphTransformer Epoch 635/1000 - Loss = 1.2530\n",
            "GraphTransformer Epoch 640/1000 - Loss = 1.2611\n",
            "GraphTransformer Epoch 645/1000 - Loss = 1.2572\n",
            "GraphTransformer Epoch 650/1000 - Loss = 1.2507\n",
            "GraphTransformer Epoch 655/1000 - Loss = 1.2547\n",
            "GraphTransformer Epoch 660/1000 - Loss = 1.2544\n",
            "GraphTransformer Epoch 665/1000 - Loss = 1.2572\n",
            "GraphTransformer Epoch 670/1000 - Loss = 1.2596\n",
            "GraphTransformer Epoch 675/1000 - Loss = 1.2564\n",
            "GraphTransformer Epoch 680/1000 - Loss = 1.2580\n",
            "GraphTransformer Epoch 685/1000 - Loss = 1.2510\n",
            "GraphTransformer Epoch 690/1000 - Loss = 1.2553\n",
            "GraphTransformer Epoch 695/1000 - Loss = 1.2563\n",
            "GraphTransformer Epoch 700/1000 - Loss = 1.2533\n",
            "GraphTransformer Epoch 705/1000 - Loss = 1.2503\n",
            "GraphTransformer Epoch 710/1000 - Loss = 1.2590\n",
            "GraphTransformer Epoch 715/1000 - Loss = 1.2508\n",
            "GraphTransformer Epoch 720/1000 - Loss = 1.2559\n",
            "GraphTransformer Epoch 725/1000 - Loss = 1.2536\n",
            "GraphTransformer Epoch 730/1000 - Loss = 1.2513\n",
            "GraphTransformer Epoch 735/1000 - Loss = 1.2520\n",
            "GraphTransformer Epoch 740/1000 - Loss = 1.2493\n",
            "GraphTransformer Epoch 745/1000 - Loss = 1.2512\n",
            "GraphTransformer Epoch 750/1000 - Loss = 1.2484\n",
            "GraphTransformer Epoch 755/1000 - Loss = 1.2422\n",
            "GraphTransformer Epoch 760/1000 - Loss = 1.2417\n",
            "GraphTransformer Epoch 765/1000 - Loss = 1.2534\n",
            "GraphTransformer Epoch 770/1000 - Loss = 1.2417\n",
            "GraphTransformer Epoch 775/1000 - Loss = 1.2418\n",
            "GraphTransformer Epoch 780/1000 - Loss = 1.2456\n",
            "GraphTransformer Epoch 785/1000 - Loss = 1.2451\n",
            "GraphTransformer Epoch 790/1000 - Loss = 1.2406\n",
            "GraphTransformer Epoch 795/1000 - Loss = 1.2349\n",
            "GraphTransformer Epoch 800/1000 - Loss = 1.2466\n",
            "GraphTransformer Epoch 805/1000 - Loss = 1.2409\n",
            "GraphTransformer Epoch 810/1000 - Loss = 1.2371\n",
            "GraphTransformer Epoch 815/1000 - Loss = 1.2381\n",
            "GraphTransformer Epoch 820/1000 - Loss = 1.2448\n",
            "GraphTransformer Epoch 825/1000 - Loss = 1.2466\n",
            "GraphTransformer Epoch 830/1000 - Loss = 1.2476\n",
            "GraphTransformer Epoch 835/1000 - Loss = 1.2328\n",
            "GraphTransformer Epoch 840/1000 - Loss = 1.2462\n",
            "GraphTransformer Epoch 845/1000 - Loss = 1.2392\n",
            "GraphTransformer Epoch 850/1000 - Loss = 1.2341\n",
            "GraphTransformer Epoch 855/1000 - Loss = 1.2385\n",
            "GraphTransformer Epoch 860/1000 - Loss = 1.2380\n",
            "GraphTransformer Epoch 865/1000 - Loss = 1.2415\n",
            "GraphTransformer Epoch 870/1000 - Loss = 1.2302\n",
            "GraphTransformer Epoch 875/1000 - Loss = 1.2352\n",
            "GraphTransformer Epoch 880/1000 - Loss = 1.2337\n",
            "GraphTransformer Epoch 885/1000 - Loss = 1.2278\n",
            "GraphTransformer Epoch 890/1000 - Loss = 1.2367\n",
            "GraphTransformer Epoch 895/1000 - Loss = 1.2270\n",
            "GraphTransformer Epoch 900/1000 - Loss = 1.2363\n",
            "GraphTransformer Epoch 905/1000 - Loss = 1.2363\n",
            "GraphTransformer Epoch 910/1000 - Loss = 1.2365\n",
            "GraphTransformer Epoch 915/1000 - Loss = 1.2303\n",
            "GraphTransformer Epoch 920/1000 - Loss = 1.2305\n",
            "GraphTransformer Epoch 925/1000 - Loss = 1.2335\n",
            "GraphTransformer Epoch 930/1000 - Loss = 1.2331\n",
            "GraphTransformer Epoch 935/1000 - Loss = 1.2347\n",
            "GraphTransformer Epoch 940/1000 - Loss = 1.2473\n",
            "GraphTransformer Epoch 945/1000 - Loss = 1.2367\n",
            "GraphTransformer Epoch 950/1000 - Loss = 1.2338\n",
            "GraphTransformer Epoch 955/1000 - Loss = 1.2310\n",
            "GraphTransformer Epoch 960/1000 - Loss = 1.2298\n",
            "GraphTransformer Epoch 965/1000 - Loss = 1.2302\n",
            "GraphTransformer Epoch 970/1000 - Loss = 1.2391\n",
            "GraphTransformer Epoch 975/1000 - Loss = 1.2400\n",
            "GraphTransformer Epoch 980/1000 - Loss = 1.2328\n",
            "GraphTransformer Epoch 985/1000 - Loss = 1.2315\n",
            "GraphTransformer Epoch 990/1000 - Loss = 1.2352\n",
            "GraphTransformer Epoch 995/1000 - Loss = 1.2285\n",
            "GraphTransformer Epoch 1000/1000 - Loss = 1.2401\n",
            "GraphTransformer: Hits@20 = 0.0000\n",
            "\n",
            "==== FINAL RESULTS ====\n",
            "GCN Hits@20       = 0.0007\n",
            "GraphSAGE Hits@20 = 0.0000\n",
            "Transformer Hits@20 = 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def debug_evaluator(model):\n",
        "    model = model.to(device)                # ✅ move model to cuda\n",
        "    model.eval()\n",
        "\n",
        "    z = model.encode(data.edge_index)       # now both live on GPU\n",
        "\n",
        "    pos_scores = model.decode(z, test_pos).view(-1).cpu()\n",
        "\n",
        "    neg_test = negative_sampling(\n",
        "        edge_index=data.edge_index.cpu(),   # ✅ negative_sampling works on CPU\n",
        "        num_nodes=num_nodes,\n",
        "        num_neg_samples=test_pos.size(0),\n",
        "    )\n",
        "    neg_test = neg_test.to(device)          # ✅ then move back to CUDA\n",
        "    neg_scores = model.decode(z, neg_test).view(-1).cpu()\n",
        "\n",
        "    result = evaluator.eval({\n",
        "        'y_pred_pos': pos_scores,\n",
        "        'y_pred_neg': neg_scores,\n",
        "    })\n",
        "\n",
        "    print(\"Returned evaluator keys:\", result.keys())\n",
        "    return result\n",
        "\n",
        "debug_evaluator(GCN(32))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVxESuFCsrX0",
        "outputId": "6345326b-ce54-4b4b-80a0-0e013c38b836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Returned evaluator keys: dict_keys(['hits@20'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hits@20': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}